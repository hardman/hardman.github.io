{"meta":{"title":"hardman的blog","subtitle":null,"description":"iOS，web，游戏技术探索","author":"hardman","url":"https://hardman.github.io"},"pages":[],"posts":[{"title":"1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） ","slug":"1小时学会：最简单的iOS直播推流（十一）sps-amp-pps和AudioSpecificConfig介绍（完结）","date":"2017-11-23T04:22:25.000Z","updated":"2018-07-19T07:13:09.421Z","comments":true,"path":"2017/11/23/1小时学会：最简单的iOS直播推流（十一）sps-amp-pps和AudioSpecificConfig介绍（完结）/","link":"","permalink":"https://hardman.github.io/2017/11/23/1小时学会：最简单的iOS直播推流（十一）sps-amp-pps和AudioSpecificConfig介绍（完结）/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 简述sps/pps/AudioSpecificConfig前文中已经多次提到过sps&amp;pps/AudioSpecificConfig。 sps&amp;pps是h264中的概念，它包含了一些编码信息，如profile，图像尺寸等信息。在flv中，包含sps&amp;pps的部分被称为 AVC Sequence header（即AVCDecoderConfigurationRecord，参考ISO-14496-15 AVC file format）。 AudioSpecificConfig是aac中的概念，它包含了音频信息，如采样率，声道数等信息。在flv中包含AudioSpecificConfig的部分被称为 AAC Sequence header（即AudioSpecificConfig，参考ISO-14496-3 Audio）。 这两种数据格式可参考标准文档或者网络上的博文，这里只介绍一下在硬编码/软编码的情况下，如何获取并处理这些数据。 可以看出，这两个概念其实就是编码的一个配置文件，保存的是后续音视频数据的一些公共属性。 sps&amp;ppsh264编码后，能够直接获取sps&amp;pps数据。 软编码获取sps&amp;pps数据的代码在aw_x264.c中1234567891011121314151617181920212223242526//软编码x264获取sps&amp;pps数据static void aw_encode_x264_header(aw_x264_context *aw_ctx)&#123; //主要就是libx264中的此方法 x264_encoder_headers(aw_ctx-&gt;x264_handler, &amp;aw_ctx-&gt;nal, &amp;aw_ctx-&gt;nal_count); //将获取到的sps&amp;pps数据取出来，保存到aw_ctx-&gt;sps_pps_data中 //保存sps pps data uint8_t *sps_bytes = NULL; int8_t sps_len = 0; uint8_t *pps_bytes = NULL; int8_t pps_len = 0; int i = 0; for (; i &lt; aw_ctx-&gt;nal_count; i++) &#123; if (aw_ctx-&gt;nal[i].i_type == NAL_SPS) &#123; sps_bytes = (uint8_t *)aw_ctx-&gt;nal[i].p_payload + 4; sps_len = aw_ctx-&gt;nal[i].i_payload - 4; &#125;else if(aw_ctx-&gt;nal[i].i_type == NAL_PPS)&#123; pps_bytes = (uint8_t *)aw_ctx-&gt;nal[i].p_payload + 4; pps_len = aw_ctx-&gt;nal[i].i_payload - 4; &#125; &#125; aw_data *avc_decoder_record = aw_create_sps_pps_data(sps_bytes, sps_len, pps_bytes, pps_len); memcpy_aw_data(&amp;aw_ctx-&gt;sps_pps_data, avc_decoder_record-&gt;data, avc_decoder_record-&gt;size); free_aw_data(&amp;avc_decoder_record);&#125; 硬编码的sps&amp;pps数据能够通过关键帧获取。代码在AWHWH264Encoder.m中 123456789101112131415161718192021222324252627282930//硬编码h264获取sps&amp;pps数据static void vtCompressionSessionCallback (void * CM_NULLABLE outputCallbackRefCon, void * CM_NULLABLE sourceFrameRefCon, OSStatus status, VTEncodeInfoFlags infoFlags, CM_NULLABLE CMSampleBufferRef sampleBuffer )&#123; ... ... ... ... //是否是关键帧，关键帧和非关键帧要区分清楚。推流时也要注明。 BOOL isKeyFrame = !CFDictionaryContainsKey( (CFArrayGetValueAtIndex(CMSampleBufferGetSampleAttachmentsArray(sampleBuffer, true), 0)), kCMSampleAttachmentKey_NotSync); //首先获取sps 和pps //sps pss 也是h264的一部分，可以认为它们是特别的h264视频帧，保存了h264视频的一些必要信息。 //没有这部分数据h264视频很难解析出来。 //数据处理时，sps pps 数据可以作为一个普通h264帧，放在h264视频流的最前面。 BOOL needSpsPps = NO; //这里判断一下只取一次sps&amp;pps即可 if (!encoder.spsPpsData) &#123; if (isKeyFrame) &#123; //获取avcC，这就是我们想要的sps和pps数据。 //如果保存到文件中，需要将此数据前加上 [0 0 0 1] 4个字节，写入到h264文件的最前面。 //如果推流，将此数据放入flv数据区即可。 CMFormatDescriptionRef sampleBufFormat = CMSampleBufferGetFormatDescription(sampleBuffer); NSDictionary *dict = (__bridge NSDictionary *)CMFormatDescriptionGetExtensions(sampleBufFormat); encoder.spsPpsData = dict[@&quot;SampleDescriptionExtensionAtoms&quot;][@&quot;avcC&quot;]; &#125; needSpsPps = YES; &#125; ... ... ... ... 成功获取sps&amp;pps数据后，可通过aw_sw_x264_encoder.c中的方法aw_encoder_create_sps_pps_tag创建对应的video tag，之后可直接像普通video tag一样发送。 12345678910111213141516171819//创建sps_pps_tagextern aw_flv_video_tag *aw_encoder_create_sps_pps_tag(aw_data *sps_pps_data)&#123; //创建普通video tag aw_flv_video_tag *sps_pps_tag = aw_sw_encoder_create_flv_video_tag(); //关键帧 sps_pps_tag-&gt;frame_type = aw_flv_v_frame_type_key; //package类型，固定的写0即可 sps_pps_tag-&gt;h264_package_type = aw_flv_v_h264_packet_type_seq_header; //cts写0 sps_pps_tag-&gt;h264_composition_time = 0; //sps&amp;pps数据，数据上同真实video tag的h264数据放同一个位置 sps_pps_tag-&gt;config_record_data = copy_aw_data(sps_pps_data); //pts写0 sps_pps_tag-&gt;common_tag.timestamp = 0; //数据总长度 sps_pps_tag-&gt;common_tag.data_size = sps_pps_data-&gt;size + 11 + sps_pps_tag-&gt;common_tag.header_size; //返回 return sps_pps_tag;&#125; AudioSpecificConfigaac软编码库faac初始化之后，能够直接获取AudioSpecificConfig数据，在aw_faac.c中。 123456789101112131415161718static void aw_open_faac_enc_handler(aw_faac_context *faac_ctx)&#123; //开启faac faac_ctx-&gt;faac_handler = faacEncOpen(faac_ctx-&gt;config.sample_rate, faac_ctx-&gt;config.channel_count, &amp;faac_ctx-&gt;max_input_sample_count, &amp;faac_ctx-&gt;max_output_byte_count); ... ... ... ... //配置好faac faacEncSetConfiguration(faac_ctx-&gt;faac_handler, faac_config); //主要通过此方法获取AudioSpecificConfig，audio_specific_data就是想要的数据 uint8_t *audio_specific_data = NULL; unsigned long audio_specific_data_len = 0; faacEncGetDecoderSpecificInfo(faac_ctx-&gt;faac_handler, &amp;audio_specific_data, &amp;audio_specific_data_len); ... ... &#125; 另外，AudioSpecificConfig数据结构很简单，可以自己简单构造一份。可参考AWHWAACEncoder.m中的createAudioSpecificConfigFlvTag函数。 1234567891011121314151617-(aw_flv_audio_tag *)createAudioSpecificConfigFlvTag&#123; //AudioSpecificConfig中包含3种元素：profile，sampleRate，channelCount //结构是：profile(5bit)-sampleRate(4bit)-channelCount(4bit)-空(3bit) uint8_t profile = kMPEG4Object_AAC_LC; uint8_t sampleRate = 4; uint8_t chanCfg = 1; uint8_t config1 = (profile &lt;&lt; 3) | ((sampleRate &amp; 0xe) &gt;&gt; 1); uint8_t config2 = ((sampleRate &amp; 0x1) &lt;&lt; 7) | (chanCfg &lt;&lt; 3); //写入config_data中 aw_data *config_data = NULL; data_writer.write_uint8(&amp;config_data, config1); data_writer.write_uint8(&amp;config_data, config2); ... ... ... ...&#125; 拿到AudioSpecificConfig数据后，可通过aw_sw_faac_encoder.c中的aw_encoder_create_audio_specific_config_tag来创建对应的flv audio tag，之后可像正常audio tag一样发送。 12345678910111213extern aw_flv_audio_tag *aw_encoder_create_audio_specific_config_tag(aw_data *audio_specific_config_data, aw_faac_config *faac_config)&#123; //创建普通的audio tag aw_flv_audio_tag *audio_tag = aw_sw_encoder_create_flv_audio_tag(faac_config); //AudioSpecificConfig数据，同正常的audio tag在相同位置 audio_tag-&gt;config_record_data = copy_aw_data(audio_specific_config_data); //时间戳0 audio_tag-&gt;common_tag.timestamp = 0; //整个tag长度 audio_tag-&gt;common_tag.data_size = audio_specific_config_data-&gt;size + 11 + audio_tag-&gt;common_tag.header_size; return audio_tag;&#125; rtmp连接成功后，一定要先发送sps&amp;pps和AudioSpecificConfig这两个数据对应的tag，否则视频是播放不出来的。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 ","slug":"1小时学会：最简单的iOS直播推流（十）librtmp使用介绍","date":"2017-11-23T04:21:27.000Z","updated":"2018-07-19T07:12:49.761Z","comments":true,"path":"2017/11/23/1小时学会：最简单的iOS直播推流（十）librtmp使用介绍/","link":"","permalink":"https://hardman.github.io/2017/11/23/1小时学会：最简单的iOS直播推流（十）librtmp使用介绍/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive rtmp(一般大写，小写会被认为英文不好或不专业，iOS开发者对这一点更为敏感)协议是Adobe公司为Flash视频的实时传输开发的一个开放协议。 本文不探究rtmp协议的原理，只是从代码角度来看，客户端如何使用librtmp完成推流功能。 librtmp项目内使用的librtmp是使用rtmpdump编译的。如果遇到代码上的疑问可以通过阅读rtmpdump的源码寻找答案。 代码解析外部接口rtmp相关代码在aw_rtmp.c和aw_rtmp.h中。对外接口包含一个context和3个函数：123456789101112131415161718192021222324252627//aw_rtmp_context是一个context，用于存储一些外部传入及内部共享的变量。//写成context统一管理，否则就要写很多全局变量了。typedef struct aw_rtmp_context&#123; //rtmp url char rtmp_url[256]; //librtmp 中的结构体，作为RTMP连接上下文 RTMP *rtmp; ... ... //外部状态检测 //状态变化回调，注意，不要在状态回调中做释放aw_rtmp_context的操作。 //如果非要释放，请延迟一帧。 aw_rtmp_state_changed_cb state_changed_cb; //当前状态 aw_rtmp_state rtmp_state;&#125; aw_rtmp_context;//打开rtmpextern int aw_rtmp_open(aw_rtmp_context *ctx);//写入数据extern int aw_rtmp_write(aw_rtmp_context *ctx, const char *buf, int size);//关闭rtmpextern int aw_rtmp_close(aw_rtmp_context *ctx); 3个主要函数分别是：打开，写入数据，关闭。除此之外，对于外部调用者来说，最重要的是要监听rtmp连接的各种状态来调整上层逻辑。而状态回调就在 aw_rtmp_context中。 项目中，初始化 &amp; 关闭rtmp的代码在 aw_streamer.c 中1234567891011121314151617//初始化rtmp连接static int8_t aw_steamer_open_rtmp_context()&#123; //创建context 传入rtmpurl及状态回调 if (!s_rtmp_ctx) &#123; s_rtmp_ctx = alloc_aw_rtmp_context(s_rtmp_url, aw_streamer_rtmp_state_changed_callback); &#125; //open return aw_rtmp_open(s_rtmp_ctx);&#125;//关闭rtmp连接static void aw_streamer_close_rtmp_context()&#123; if (s_rtmp_ctx) &#123; aw_rtmp_close(s_rtmp_ctx); &#125; aw_log(&quot;[d] closed rtmp context&quot;);&#125; 发送数据的代码在aw_streamer.c中： 1234567static void aw_streamer_send_flv_tag_to_rtmp(aw_flv_common_tag *common_tag)&#123; ... ... aw_rtmp_write(s_rtmp_ctx, (const char *)s_output_buf-&gt;data, s_output_buf-&gt;size); ... ...&#125; 打开rtmp123456789101112131415161718192021222324252627282930313233343536373839//打开rtmp，都是固定套路。int aw_rtmp_open(aw_rtmp_context *ctx)&#123; ... ... //初始化 ctx-&gt;rtmp = RTMP_Alloc(); RTMP_Init(ctx-&gt;rtmp); //连接超时 ctx-&gt;rtmp-&gt;Link.timeout = 1; //设置url if (!RTMP_SetupURL(ctx-&gt;rtmp, ctx-&gt;rtmp_url)) &#123; AWLog(&quot;[error ] aw rtmp setup url = %s\\n&quot;, ctx-&gt;rtmp_url); recode = -2; goto FAILED; &#125; //可写 RTMP_EnableWrite(ctx-&gt;rtmp); //buffer长度 RTMP_SetBufferMS(ctx-&gt;rtmp, 0); //开始连接 if (!RTMP_Connect(ctx-&gt;rtmp, NULL)) &#123; recode = -3; goto FAILED; &#125; //连接 if (!RTMP_ConnectStream(ctx-&gt;rtmp, 0)) &#123; recode = -4; goto FAILED; &#125; return 1;FAILED: //若中间环节出错，断开连接 aw_rtmp_close(ctx); return !recode;&#125; rtmp写入（发送）数据12345678int aw_rtmp_write(aw_rtmp_context *ctx, const char *buf, int size)&#123; ... ... //RTMP_Write内部有时会排出SIGPIPE信号，在这里处理一下 signal(SIGPIPE, SIG_IGN); int write_ret = RTMP_Write(ctx-&gt;rtmp, buf, size); ... ... return write_ret;&#125; rtmp关闭12345678int aw_rtmp_close(aw_rtmp_context *ctx)&#123; ... ... //主要这两句 RTMP_Close(ctx-&gt;rtmp); RTMP_Free(ctx-&gt;rtmp); ... ... return 1;&#125; librtmp库使用方法介绍完毕。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 ","slug":"1小时学会：最简单的iOS直播推流（九）flv-编码与音视频时间戳同步","date":"2017-11-23T04:20:18.000Z","updated":"2018-07-19T07:12:12.151Z","comments":true,"path":"2017/11/23/1小时学会：最简单的iOS直播推流（九）flv-编码与音视频时间戳同步/","link":"","permalink":"https://hardman.github.io/2017/11/23/1小时学会：最简单的iOS直播推流（九）flv-编码与音视频时间戳同步/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 前文介绍了如何获取音视频的aac/h264数据，那么如何将数据写入rtmp流中呢？rtmp最初是Adobe Flash用于音视频播放的一个实时传输协议。而flv正是Adobe推出的一个视频格式，因此rtmp协议支持flv视频流。这里可以我们把获取的aac/h264的数据，直接转成flv格式的视频帧，然后按照时间戳依次发送给服务端即可。 flv格式简介flv总体来说是一个简单的视频格式，它包含2部分：header 和 body。 header是固定格式的数据，表示本文件是一个flv文件。header的长度是9个字节。 header后面紧跟着body数据。body是由一个一个称为的tag数据组成。tag其实就是一个固定格式的数据块，构造方式同header类似，只是叫法不同而已。 tag分为3种。script tag，video tag，audio tag。script tag是flv的第一个tag，用于放一些视频信息的，比如duration，width，height等。script tag对于flv格式的视频文件比较重要，对于rtmp来说，可以不写入script tag。video tag是视频数据的封装，也就是我们获取的h264数据基础之上，增加一些flv特定的数据。audio tag同video tag类似，是acc数据的封装。 代码解析flv相关代码在 aw_encode_flv.h和aw_encode_flv.c中。此模块提供了flv编码(aac+h264)功能。 这个模块的暴露给外部的api为2部分：123456789101112131415161718//一部分是创建flv的方法//写入headerextern void aw_write_flv_header(aw_data **flv_data);//写入flv tagextern void aw_write_flv_tag(aw_data **flv_data, aw_flv_common_tag *common_tag);//第二部分是所有tag的构造//script tagextern aw_flv_script_tag *alloc_aw_flv_script_tag();extern void free_aw_flv_script_tag(aw_flv_script_tag **);//audio tagextern aw_flv_audio_tag *alloc_aw_flv_audio_tag();extern void free_aw_flv_audio_tag(aw_flv_audio_tag **);//video tagextern aw_flv_video_tag *alloc_aw_flv_video_tag();extern void free_aw_flv_video_tag(aw_flv_video_tag **); 外部使用时，可根据具体数据先创建不同的tag，填充好各个数据，然后使用aw_write_flv_tag方法将tag写入aw_data中。可用上述方法可以构造出完整的flv文件。 aw_dataaw_data是为了方便文件数据的读取/写入和管理而创建的工具模块。此模块已处理了大端小端差异，能够让文件读写更加方便快捷。相关代码在aw_data.h / aw_data.c中。 flv header12345678910111213141516extern void aw_write_flv_header(aw_data **flv_data)&#123; uint8_t f = &apos;F&apos;, l = &apos;L&apos;, v = &apos;V&apos;,//FLV version = 1,//固定值 av_flag = 5;//5表示av，5表示只有a，1表示只有v uint32_t flv_header_len = 9;//header固定长度为9 data_writer.write_uint8(flv_data, f); data_writer.write_uint8(flv_data, l); data_writer.write_uint8(flv_data, v); data_writer.write_uint8(flv_data, version); data_writer.write_uint8(flv_data, av_flag); data_writer.write_uint32(flv_data, flv_header_len); //first previous tag size 根据flv协议，每个tag后要写入当前tag的size，称为previous tag size，header后面需要写入4字节空数据。 data_writer.write_uint32(flv_data, 0);&#125; flv body 注意如果是要构造flv文件，写入header之后就可以写入script tag了。如果是使用rtmp协议，则无需构造header，也无需script tag。可直接写入 video tag和audio tag。若使用rtmp协议必须在首帧写入AVCDecoderConfigurationRecord (包含sps pps数据)和 AudioSpecificConfig，否则服务端无法正常解析音视频数据。 flv的body是由一个接一个的tag构成的。一个flv tag分为3部分：tag header + tag body + tag data size。 12345678extern void aw_write_flv_tag(aw_data **flv_data, aw_flv_common_tag *common_tag)&#123; //写入header aw_write_tag_header(flv_data, common_tag); //写入body aw_write_tag_body(flv_data, common_tag); //写入data size aw_write_tag_data_size(flv_data, common_tag);&#125; tag header123456789101112static void aw_write_tag_header(aw_data **flv_data, aw_flv_common_tag *common_tag)&#123; //header 长度为固定11个字节 //写入tag type，video：9 audio：8 script：18 data_writer.write_uint8(flv_data, common_tag-&gt;tag_type); //写入body的size(data_size为整个tag的长度) data_writer.write_uint24(flv_data, common_tag-&gt;data_size - 11); //写入时间戳 data_writer.write_uint24(flv_data, common_tag-&gt;timestamp); data_writer.write_uint8(flv_data, common_tag-&gt;timestamp_extend); //写入stream id为0 data_writer.write_uint24(flv_data, common_tag-&gt;stream_id);&#125; script tag body12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static void aw_write_script_tag_body(aw_data **flv_data, aw_flv_script_tag *script_tag)&#123; //script tag写入规则为：类型-内容-类型-内容...类型-内容 //类型是1个字节整数，可取12种值： // 0 = Number type // 1 = Boolean type // 2 = String type // 3 = Object type // 4 = MovieClip type // 5 = Null type // 6 = Undefined type // 7 = Reference type // 8 = ECMA array type // 10 = Strict array type // 11 = Date type // 12 = Long string type // 比如：如果类型是字符串，那么先写入1个字节表类型的2。另，写入真正的字符串前，需要写入2个字节的字符串长度。 // data_writer.write_string能够在写入字符串前，先写入字符串长度，此函数第三个参数表示用多少字节来存储字符串长度。 // script tag 的结构基本上是固定的，首先写入一个字符串: onMetaData，然后写入一个数组。 // 写入数组需要先写入数组编号1字节：8，然后写入数组长度4字节：11。 // 数组同OC的Dictionary类似，可写入一个字符串+一个value。 // 所以每个数组元素可先写入一个字符串，然后写入一个Number Type，再写入具体的数值。 // 结束时需写入3个字节的0x000009表示数组结束。 // 下面代码中的duration/width/filesize均遵循此规则。 //2表示类型，字符串 data_writer.write_uint8(flv_data, 2); data_writer.write_string(flv_data, &quot;onMetaData&quot;, 2); //数组类型：8 data_writer.write_uint8(flv_data, 8); //数组长度：11 data_writer.write_uint32(flv_data, 11); //写入duration 0表示double，1表示uint8 data_writer.write_string(flv_data, &quot;duration&quot;, 2); data_writer.write_uint8(flv_data, 0); data_writer.write_double(flv_data, script_tag-&gt;duration); //写入width data_writer.write_string(flv_data, &quot;width&quot;, 2); data_writer.write_uint8(flv_data, 0); data_writer.write_double(flv_data, script_tag-&gt;width); ... ... ... //写入file_size data_writer.write_string(flv_data, &quot;filesize&quot;, 2); data_writer.write_uint8(flv_data, 0); data_writer.write_double(flv_data, script_tag-&gt;file_size); //3字节的0x9表示数组结束 data_writer.write_uint24(flv_data, 9);&#125; video tag body12345678910111213141516171819202122232425262728293031323334static void aw_write_video_tag_body(aw_data **flv_data, aw_flv_video_tag *video_tag)&#123; // video tag body 结构是这样的： // frame_type(4bit) + codec_id(4bit) + h264_package_type(8bit) + h264_composition_time(24bit) + video_tag_data(many bits) // frame_type 表示是否关键帧，关键帧为1，非关键帧为2（当然还有更多取值，请参考[flv协议](https://wuyuans.com/img/2012/08/video_file_format_spec_v10.rar) // codec_id 表示视频协议：h264是7 h263是2。 // h264_package_type表示视频帧数据的类型，2种取值：sequence header（也就是前面说的 sps pps 数据，rtmp要求首帧发送此数据，也称为AVCDecoderConfigurationRecord），另一种为nalu，正常的h264视频帧。 // h264_compsition_time：cts是pts与dts的差值，flv中的timestamp表示的应该是pts。如果h264数据中不包含B帧，那么此数据可传0。 // video_tag_data 即纯264数据。 uint8_t video_header = 0; video_header |= video_tag-&gt;frame_type &lt;&lt; 4 &amp; 0xf0; video_header |= video_tag-&gt;codec_id; data_writer.write_uint8(flv_data, video_header); if (video_tag-&gt;codec_id == aw_flv_v_codec_id_H264) &#123; data_writer.write_uint8(flv_data, video_tag-&gt;h264_package_type); data_writer.write_uint24(flv_data, video_tag-&gt;h264_composition_time); &#125; switch (video_tag-&gt;h264_package_type) &#123; case aw_flv_v_h264_packet_type_seq_header: &#123; data_writer.write_bytes(flv_data, video_tag-&gt;config_record_data-&gt;data, video_tag-&gt;config_record_data-&gt;size); break; &#125; case aw_flv_v_h264_packet_type_nalu: &#123; data_writer.write_bytes(flv_data, video_tag-&gt;frame_data-&gt;data, video_tag-&gt;frame_data-&gt;size); break; &#125; case aw_flv_v_h264_packet_type_end_of_seq: &#123; //nothing break; &#125; &#125;&#125; audio tag body12345678910111213141516171819202122232425262728293031static void aw_write_audio_tag_body(aw_data **flv_data, aw_flv_audio_tag *audio_tag)&#123; // audio tag body的结构是这样的： // sound_format(4bit) + sound_rate(sample_rate)(2bit) + sound_size(sample_size)(1bit) + sound_type(1bit) + aac_packet_type(8bit) + aac_data(many bits) // sound_format 表示声音格式，2表示mp3，10表示aac，一般是aac // sound_rate 采样率，表示1秒钟采集多少个样本，可选4个值，0表示5.5kHZ，1表示11kHZ，2表示22kHZ，3表示44kHZ，一般是3。 // sound_size 采样尺寸，单个样本的size。2个选择，0表示8bit，1表示16bit。 // 直观上看，采样率和采样尺寸应该和质量有一定关系。采样率高，采样尺寸大效果应该会好，但是生成的数据量也大。 // sound_type 表示声音类型，0表示单声道，1表示立体声。(立体声有2条声道)。 // aac_packet_type表示aac数据类型，有2种选择：0表示sequence header，即 必须首帧发送的数据(AudioSpecificConfig)，1表示正常的aac数据。 uint8_t audio_header = 0; audio_header |= audio_tag-&gt;sound_format &lt;&lt; 4 &amp; 0xf0; audio_header |= audio_tag-&gt;sound_rate &lt;&lt; 2 &amp; 0xc; audio_header |= audio_tag-&gt;sound_size &lt;&lt; 1 &amp; 0x2; audio_header |= audio_tag-&gt;sound_type &amp; 0x1; data_writer.write_uint8(flv_data, audio_header); if (audio_tag-&gt;sound_format == aw_flv_a_codec_id_AAC) &#123; data_writer.write_uint8(flv_data, audio_tag-&gt;aac_packet_type); &#125; switch (audio_tag-&gt;aac_packet_type) &#123; case aw_flv_a_aac_package_type_aac_sequence_header: &#123; data_writer.write_bytes(flv_data, audio_tag-&gt;config_record_data-&gt;data, audio_tag-&gt;config_record_data-&gt;size); break; &#125; case aw_flv_a_aac_package_type_aac_raw: &#123; data_writer.write_bytes(flv_data, audio_tag-&gt;frame_data-&gt;data, audio_tag-&gt;frame_data-&gt;size); break; &#125; &#125;&#125; tag data size根据flv协议，每个flv tag结束时，需要写入此tag的全部长度：header+body的长度，header长度固定为11字节，而body的长度可通过上面构造body时写入的数据进行计算。123static void aw_write_tag_data_size(aw_data **flv_data, aw_flv_common_tag *common_tag)&#123; data_writer.write_uint32(flv_data, common_tag-&gt;data_size);&#125; 上面的data_size由外部使用此模块的函数，在创建tag时计算出来的。可以看aw_sw_faac_encoder.c中的aw_encoder_create_audio_tag方法：12345678extern aw_flv_audio_tag *aw_encoder_create_audio_tag(int8_t *aac_data, long len, uint32_t timeStamp, aw_faac_config *faac_cfg)&#123; aw_flv_audio_tag *audio_tag = aw_sw_encoder_create_flv_audio_tag(faac_cfg); ... ... //此处计算的data_size长度为 11(tag header size) + body header size(即下面的header_size，表示body中除去aac data的部分) + aac data size audio_tag-&gt;common_tag.data_size = audio_tag-&gt;frame_data-&gt;size + 11 + audio_tag-&gt;common_tag.header_size; return audio_tag;&#125; 这是本项目的处理方式。当然data size也可以在写入header和body时，同步计算出来。 flv时间戳flv的tag中有2个字段表示时间戳，一个是 timestamp(pts)，一个是Composition Time(cts)。pts表示展示时间戳，表示这一帧什么时候展示。说cts之前有必要介绍一下dts，dts表示解码时间戳。我们知道h264中有3种视频帧，I帧，P帧，B帧。I和P帧不必说。因为B帧的存在，可能会令后面的视频帧先于前面的视频帧解析，这样就需要在视频帧信息中保存dts。flv中的cts可以做这件事情，cts = pts - dts。 另一个问题是，rtmp中的flv时间戳有一个规则就是，音频+视频帧须按照pts递增顺序发送。因为音频和视频有各自的帧率，每个音视频帧可计算出各自的时间戳。由于音频和视频在不同的线程中编码，编码后的音视频会合并到相同的线程中发送。因为编码速度等各种原因，编码后的数据合并到相同线程时，可能并不是按照时间戳升序排列的。 为了保证排序，有2种办法解决此问题: 将数据缓存起来，每次发送前都保证发送的是最早的数据帧。 以音频（或视频）为主，一旦遇到视频（或音频）帧时间戳小于已经发送的时间戳，则调整视频（或音频）帧时间戳。 推流时保存发送的flv文件根据本文介绍，我们可以把发送到rtmp服务器的数据保存到本地flv文件。可以修改aw_streamer.c文件。 当调用aw_streamer_open_rtmp_context时创建aw_data，并写入flv header和flv script tag。 调用aw_streamer_send_video_data和aw_streamer_send_audio_data时，将video tag和audio tag写入aw_data中。 当调用aw_streamer_close_rtmp_context时，将aw_data写入到本地文件，保存成flv格式，然后释放aw_data。 至此，flv编码介绍完毕。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 ","slug":"1小时学会：最简单的iOS直播推流（八）h264-aac-软编码","date":"2017-11-23T04:19:04.000Z","updated":"2018-07-19T07:09:07.719Z","comments":true,"path":"2017/11/23/1小时学会：最简单的iOS直播推流（八）h264-aac-软编码/","link":"","permalink":"https://hardman.github.io/2017/11/23/1小时学会：最简单的iOS直播推流（八）h264-aac-软编码/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 软编码包含3部分内容： 将pcm/yuv数据编码成aac/h264格式 将aac/h264数据封装成flv格式 另外无论软编码还是硬编码，最后获得的flv格式数据，需要通过rtmp协议发送至服务器。 本篇将介绍第1部分内容。另外两部分内容将在后续文章中介绍。 根据上文介绍，软编码实现，对应音频／视频编码分别为：AWSWFaacEncoder 和 AWSWX264Encoder。 这两个类只是用OC封装的一个壳，实际上使用的是 libfaac 和 libx264 进行处理。 音频软编码aw_faac.h和aw_faac.c这两个文件是对libfaac这个库使用方法的简单封装。这两个文件预期功能是，封装出一个函数，将pcm数据，转成aac数据。 faac的使用步骤： 使用 faacEncOpen 开启编码环境 配置编码属性。 使用 faacEncEncode 函数编码。 使用完毕后，调用 faacEncClose 关闭编码环境。 根据这个步骤，来看aw_faac.c文件。 faac封装第一步：开启编码环境12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/* aw_faac_context 是自己创建的结构体，用于辅助aac编码，存储了faac库的必需的数据，及一些过程变量。 它的创建及关闭请看demo中的代码，很简单，这里不需要解释。*/static void aw_open_faac_enc_handler(aw_faac_context *faac_ctx)&#123; // 开启faac // 参数依次为： // 输入 采样率(44100) 声道数(2) // 得到 最大输入样本数(1024) 最大输出字节数(2048) faac_ctx-&gt;faac_handler = faacEncOpen(faac_ctx-&gt;config.sample_rate, faac_ctx-&gt;config.channel_count, &amp;faac_ctx-&gt;max_input_sample_count, &amp;faac_ctx-&gt;max_output_byte_count); //根据最大输入样本数得到最大输入字节数 faac_ctx-&gt;max_input_byte_count = faac_ctx-&gt;max_input_sample_count * faac_ctx-&gt;config.sample_size / 8; if(!faac_ctx-&gt;faac_handler)&#123; aw_log(&quot;[E] aac handler open failed&quot;); return; &#125; //创建buffer faac_ctx-&gt;aac_buffer = aw_alloc(faac_ctx-&gt;max_output_byte_count); //获取配置 faacEncConfigurationPtr faac_config = faacEncGetCurrentConfiguration(faac_ctx-&gt;faac_handler); if (faac_ctx-&gt;config.sample_size == 16) &#123; faac_config-&gt;inputFormat = FAAC_INPUT_16BIT; &#125;else if (faac_ctx-&gt;config.sample_size == 24) &#123; faac_config-&gt;inputFormat = FAAC_INPUT_24BIT; &#125;else if (faac_ctx-&gt;config.sample_size == 32) &#123; faac_config-&gt;inputFormat = FAAC_INPUT_32BIT; &#125;else&#123; faac_config-&gt;inputFormat = FAAC_INPUT_FLOAT; &#125; //配置 faac_config-&gt;aacObjectType = LOW;//aac对象类型: LOW Main LTP faac_config-&gt;mpegVersion = MPEG4;//mpeg版本: MPEG2 MPEG4 faac_config-&gt;useTns = 1;//抗噪 faac_config-&gt;allowMidside = 0;// 是否使用mid/side编码 if(faac_ctx-&gt;config.bitrate)&#123; //每秒钟每个通道的bitrate faac_config-&gt;bitRate = faac_ctx-&gt;config.bitrate / faac_ctx-&gt;config.channel_count; &#125; faacEncSetConfiguration(faac_ctx-&gt;faac_handler, faac_config); //获取audio specific config，本系列文章中第六篇里面介绍了这个数据，它存储了aac格式的一些关键数据， //在rtmp协议中，必须将此数据在所有音频帧之前发送 uint8_t *audio_specific_data = NULL; unsigned long audio_specific_data_len = 0; faacEncGetDecoderSpecificInfo(faac_ctx-&gt;faac_handler, &amp;audio_specific_data, &amp;audio_specific_data_len); //将获取的audio specific config data 存储到faac_ctx中 if (audio_specific_data_len &gt; 0) &#123; faac_ctx-&gt;audio_specific_config_data = alloc_aw_data(0); memcpy_aw_data(&amp;faac_ctx-&gt;audio_specific_config_data, audio_specific_data, (uint32_t)audio_specific_data_len); &#125; &#125;//函数内具体参数配置，请参考：//http://wenku.baidu.com/link?url=0E9GnSo7hZ-3WmB_eXz8EfnG8NqJJJtvjrVNW7hW-VEYWW-gYBMVM-CnFSicDE-veDl2tzfL-nu2FQ8msGcCOALuT8VW1l_NjQL9Gvw5V6_ faac封装第二步：开始编码123456789101112131415161718192021222324252627282930313233343536373839404142434445/* pcm_data 为 pcm格式的音频数据 len 表示数据字节数*/extern void aw_encode_pcm_frame_2_aac(aw_faac_context *ctx, int8_t *pcm_data, long len)&#123; //判断输入参数 if (!pcm_data || len &lt;= 0) &#123; aw_log(&quot;[E] aw_encode_pcm_frame_2_aac params error&quot;); return; &#125; //清空encoded_aac_data，每次编码数据最终会存储到此字段中，所以首先清空。 reset_aw_data(&amp;ctx-&gt;encoded_aac_data); /* 下列代码根据第一步&quot;开启编码环境&quot;函数中计算的最大输入子节数 将pcm_data分割成合适的大小，使用faacEncEncode函数将pcm数据编码成aac数据。 下列代码执行完成后，编码出的aac数据将会存储到encoded_aac_data字段中。 */ long max_input_count = ctx-&gt;max_input_byte_count; long curr_read_count = 0; do&#123; long remain_count = len - curr_read_count; if (remain_count &lt;= 0) &#123; break; &#125; long read_count = 0; if (remain_count &gt; max_input_count) &#123; read_count = max_input_count; &#125;else&#123; read_count = remain_count; &#125; long input_samples = read_count * 8 / ctx-&gt;config.sample_size; int write_count = faacEncEncode(ctx-&gt;faac_handler, (int32_t * )(pcm_data + curr_read_count), (uint32_t)input_samples, (uint8_t *)ctx-&gt;aac_buffer, (uint32_t)ctx-&gt;max_output_byte_count); if (write_count &gt; 0) &#123; data_writer.write_bytes(&amp;ctx-&gt;encoded_aac_data, (const uint8_t *)ctx-&gt;aac_buffer, write_count); &#125; curr_read_count += read_count; &#125; while (curr_read_count + max_input_count &lt; len);&#125; faac封装第三步：关闭编码器：123456extern void free_aw_faac_context(aw_faac_context **context_p)&#123; ... //关闭faac编码器 faacEncClose(context-&gt;faac_handler); ...&#125; 上述代码仅仅作为faac编码器的封装，能够实现打开编码器。 真正实现编码过程的文件是：aw_sw_faac_encoder.h/aw_sw_faac_encoder.c文件 此文件的功能是：将传入的pcm数据通过aw_faac.c提供的功能，将数据转成aac数据格式，然后将aac数据格式转成flv格式，如何转成flv格式，会在后续文章介绍。 来看一下 aw_sw_faac_encoder.c文件的实现。此文件逻辑也很清晰，它实现的功能有： 开启编码器，创建一些过程变量。 将audio specific config data 转成flv帧数据。 将接收到的pcm数据，转成aac数据，然后将aac数据转成flv音频数据。 关闭编码器。 可以看出，这种类似功能性代码，一般都是三部曲：打开－使用－关闭。 下面来看代码。 音频软编码器第一步：开启编码器1234567891011121314151617181920/* faac_config：需要由上层传入相关配置属性*/extern void aw_sw_encoder_open_faac_encoder(aw_faac_config *faac_config)&#123; //是否已经开启了，避免重复开启 if (aw_sw_faac_encoder_is_valid()) &#123; aw_log(&quot;[E] aw_sw_encoder_open_faac_encoder when encoder is already inited&quot;); return; &#125; //创建配置 int32_t faac_cfg_len = sizeof(aw_faac_config); if (!s_faac_config) &#123; s_faac_config = aw_alloc(faac_cfg_len); &#125; memcpy(s_faac_config, faac_config, faac_cfg_len); //开启faac软编码 s_faac_ctx = alloc_aw_faac_context(*faac_config);&#125; 音频软编码第二步：将audio specific config data 转成flv帧数据。12345678910111213141516171819extern aw_flv_audio_tag *aw_sw_encoder_create_faac_specific_config_tag()&#123; //是否已打开编码器 if(!aw_sw_faac_encoder_is_valid())&#123; aw_log(&quot;[E] aw_sw_encoder_create_faac_specific_config_tag when audio encoder is not inited&quot;); return NULL; &#125; //创建 audio specfic config record aw_flv_audio_tag *aac_tag = aw_sw_encoder_create_flv_audio_tag(&amp;s_faac_ctx-&gt;config); //根据flv协议：audio specific data对应的 aac_packet_type 固定为 aw_flv_a_aac_package_type_aac_sequence_header 值为0 //普通的音频帧，此处值为1. aac_tag-&gt;aac_packet_type = aw_flv_a_aac_package_type_aac_sequence_header; aac_tag-&gt;config_record_data = copy_aw_data(s_faac_ctx-&gt;audio_specific_config_data); aac_tag-&gt;common_tag.timestamp = 0; aac_tag-&gt;common_tag.data_size = s_faac_ctx-&gt;audio_specific_config_data-&gt;size + 11 + aac_tag-&gt;common_tag.header_size; return aac_tag;&#125; 音频软编码器第三步：将接收到的pcm数据转成aac数据，然后将aac数据转成flv音频数据12345678910111213141516171819202122232425262728293031/* pcm_data: 传入的pcm数据 len: pcm数据长度 timestamp：flv时间戳，rtmp协议要求发送的flv音视频帧的时间戳需为均匀增加，不允许 后发送的数据时间戳 比 先发送的数据的时间戳 还要小。 aw_flv_audio_tag: 返回类型，生成的flv音频数据（flv中，每帧数据称为一个tag）。*/extern aw_flv_audio_tag *aw_sw_encoder_encode_faac_data(int8_t *pcm_data, long len, uint32_t timestamp)&#123; if (!aw_sw_faac_encoder_is_valid()) &#123; aw_log(&quot;[E] aw_sw_encoder_encode_faac_data when encoder is not inited&quot;); return NULL; &#125; //将pcm数据编码成aac数据 aw_encode_pcm_frame_2_aac(s_faac_ctx, pcm_data, len); // 使用faac编码的数据会带有7个字节的adts头。rtmp不接受此值，在此去掉前7个字节。 int adts_header_size = 7; //除去ADTS头的7字节 if (s_faac_ctx-&gt;encoded_aac_data-&gt;size &lt;= adts_header_size) &#123; return NULL; &#125; //将aac数据封装成flv音频帧。flv帧仅仅是将aac数据增加一些固定信息。并没有对aac数据进行编码操作。 aw_flv_audio_tag *audio_tag = aw_encoder_create_audio_tag((int8_t *)s_faac_ctx-&gt;encoded_aac_data-&gt;data + adts_header_size, s_faac_ctx-&gt;encoded_aac_data-&gt;size - adts_header_size, timestamp, &amp;s_faac_ctx-&gt;config); audio_count++; //返回结果 return audio_tag;&#125; 音频软编码器第四步：关闭编码器12345678910111213141516extern void aw_sw_encoder_close_faac_encoder()&#123; //避免重复关闭 if (!aw_sw_faac_encoder_is_valid()) &#123; aw_log(&quot;[E] aw_sw_encoder_close_faac_encoder when encoder is not inited&quot;); return; &#125; //是否aw_faac_context，也就关闭了faac编码环境。 free_aw_faac_context(&amp;s_faac_ctx); //释放配置数据 if (s_faac_config) &#123; aw_free(s_faac_config); s_faac_config = NULL; &#125;&#125; 到此为止，音频软编码器就介绍完了。已经成功实现了将pcm数据转成flv音频帧。 下面介绍视频软编码。套路同音频编码一致，对应的视频软编码是对x264这个库的封装。文件在aw_x264.h/aw_x264.c中。 它实现的功能如下： 初始化x264参数，打开编码环境 进行编码 关闭编码环境。 x264封装第一步：初始化x264参数，打开编码环境123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/* config 表示配置数据 aw_x264_context 是自定义结构体，用于存储x264编码重要属性及过程变量。*/extern aw_x264_context *alloc_aw_x264_context(aw_x264_config config)&#123; aw_x264_context *ctx = aw_alloc(sizeof(aw_x264_context)); memset(ctx, 0, sizeof(aw_x264_context)); //数据数据默认为 I420 if (!config.input_data_format) &#123; config.input_data_format = X264_CSP_I420; &#125; //创建handler memcpy(&amp;ctx-&gt;config, &amp;config, sizeof(aw_x264_config)); x264_param_t *x264_param = NULL; //x264参数，具体请参考：http://blog.csdn.net/table/article/details/8085115 aw_create_x264_param(ctx, &amp;x264_param); //开启编码器 aw_open_x264_handler(ctx, x264_param); aw_free(x264_param); //创建pic_in，x264内部用于存储输入图像数据的一段空间。 x264_picture_t *pic_in = aw_alloc(sizeof(x264_picture_t)); x264_picture_init(pic_in); //[注意有坑] //aw_stride是一个宏，用于将视频宽度转成16的倍数。如果不是16的倍数，有时候会编码失败（颜色缺失等）。 int alloc_width = aw_stride(config.width); x264_picture_alloc(pic_in, config.input_data_format, alloc_width, config.height); pic_in-&gt;img.i_csp = config.input_data_format; //i_stride 表示换行步长，跟plane数及格式有关，x264内部用来判定读取多少数据需要换行。 //关于yuv数据格式在第二章里面介绍过，这里再次回顾一下。 if (config.input_data_format == X264_CSP_NV12) &#123; //nv12数据包含2个plane，第一个plane存储了y数据大小为 width * height， //第二个plane存储uv数据，u和v隔位存储，数据大小为：width * (height / 2) pic_in-&gt;img.i_stride[0] = alloc_width; pic_in-&gt;img.i_stride[1] = alloc_width; pic_in-&gt;img.i_plane = 2; &#125;else if(config.input_data_format == X264_CSP_BGR || config.input_data_format == X264_CSP_RGB)&#123; //rgb数据包含一个plane，数据长度为 width * 3 * height。 pic_in-&gt;img.i_stride[0] = alloc_width * 3; pic_in-&gt;img.i_plane = 1; &#125;else if(config.input_data_format == X264_CSP_BGRA)&#123; //bgra同rgb类似 pic_in-&gt;img.i_stride[0] = alloc_width * 4; pic_in-&gt;img.i_plane = 1; &#125;else&#123;//YUV420 //yuv420即I420格式。 //包含3个plane，第一个plane存储y数据大小为width * height //第二个存储u数据，数据大小为 width * height / 4 //第三个存储v数据，数据大小为 width * height / 4 pic_in-&gt;img.i_stride[0] = alloc_width; pic_in-&gt;img.i_stride[1] = alloc_width / 2; pic_in-&gt;img.i_stride[2] = alloc_width / 2; pic_in-&gt;img.i_plane = 3; &#125; //其他数据初始化，pic_in 用于存储输入数据(yuv/rgb等数据)，pic_out用于存储输出数据(h264数据) ctx-&gt;pic_in = pic_in; ctx-&gt;pic_out = aw_alloc(sizeof(x264_picture_t)); x264_picture_init(ctx-&gt;pic_out); //编码后数据变量 ctx-&gt;encoded_h264_data = alloc_aw_data(0); ctx-&gt;sps_pps_data = alloc_aw_data(0); //获取sps pps // sps pps 数据是rtmp协议要求的必需在所有flv视频帧之前发送的一帧数据，存储了h264视频的一些关键属性。 // 具体获取方法请看demo，很简单，这里就不解释了。 aw_encode_x264_header(ctx); return ctx;&#125; x264封装第二步：开始编码1234567891011121314151617181920212223242526272829303132//编码一帧数据extern void aw_encode_yuv_frame_2_x264(aw_x264_context *aw_ctx, int8_t *yuv_frame, int len)&#123; if (len &gt; 0 &amp;&amp; yuv_frame) &#123; //将视频数据填充到pic_in中，pic_in上面已经介绍过，x264需要这样处理。 int actual_width = aw_stride(aw_ctx-&gt;config.width); //数据保存到pic_in中 if (aw_ctx-&gt;config.input_data_format == X264_CSP_NV12) &#123; aw_ctx-&gt;pic_in-&gt;img.plane[0] = (uint8_t *)yuv_frame; aw_ctx-&gt;pic_in-&gt;img.plane[1] = (uint8_t *)yuv_frame + actual_width * aw_ctx-&gt;config.height; &#125;else if(aw_ctx-&gt;config.input_data_format == X264_CSP_BGR || aw_ctx-&gt;config.input_data_format == X264_CSP_RGB)&#123; aw_ctx-&gt;pic_in-&gt;img.plane[0] = (uint8_t *)yuv_frame; &#125;else if(aw_ctx-&gt;config.input_data_format == X264_CSP_BGRA)&#123; aw_ctx-&gt;pic_in-&gt;img.plane[0] = (uint8_t *)yuv_frame; &#125;else&#123;//YUV420 aw_ctx-&gt;pic_in-&gt;img.plane[0] = (uint8_t *)yuv_frame; aw_ctx-&gt;pic_in-&gt;img.plane[1] = (uint8_t *)yuv_frame + actual_width * aw_ctx-&gt;config.height; aw_ctx-&gt;pic_in-&gt;img.plane[2] = (uint8_t *)yuv_frame + actual_width * aw_ctx-&gt;config.height * 5 / 4; &#125; //x264编码，编码后的数据存储在aw_ctx-&gt;nal中 x264_encoder_encode(aw_ctx-&gt;x264_handler, &amp;aw_ctx-&gt;nal, &amp;aw_ctx-&gt;nal_count, aw_ctx-&gt;pic_in, aw_ctx-&gt;pic_out); aw_ctx-&gt;pic_in-&gt;i_pts++; &#125; //将编码后的数据转存到encoded_h264_data中，这里面存储的就是编码好的h264视频帧了。 reset_aw_data(&amp;aw_ctx-&gt;encoded_h264_data); if (ctx-&gt;nal_count &gt; 0) &#123; int i = 0; for (; i &lt; ctx-&gt;nal_count; i++) &#123; data_writer.write_bytes(&amp;ctx-&gt;encoded_h264_data, ctx-&gt;nal[i].p_payload, ctx-&gt;nal[i].i_payload); &#125; &#125;&#125; x264封装第三步：关闭编码环境。1234567891011121314151617181920212223242526272829/* 很简单，分别释放pic_in，pic_out，x264_handler即可*/extern void free_aw_x264_context(aw_x264_context **ctx_p)&#123; aw_x264_context *ctx = *ctx_p; if (ctx) &#123; //释放pic_in if (ctx-&gt;pic_in) &#123; x264_picture_clean(ctx-&gt;pic_in); aw_free(ctx-&gt;pic_in); ctx-&gt;pic_in = NULL; &#125; //释放pic_out if (ctx-&gt;pic_out) &#123; aw_free(ctx-&gt;pic_out); ctx-&gt;pic_out = NULL; &#125; ... //关闭handler if (ctx-&gt;x264_handler) &#123; x264_encoder_close(ctx-&gt;x264_handler); ctx-&gt;x264_handler = NULL; &#125; ... &#125;&#125; 上面的代码只是对x264编码流程进行简单封装。真正实现完整转码逻辑的是在 aw_sw_x264_encoder.h/aw_sw_x264_encoder.c 中。 它实现了如下功能： 将收到的yuv数据编码成 h264格式。 生成包含sps/pps数据的flv视频帧。 将h264格式的数据转成flv视频数据。 关闭编码器。 视频软编码器第一步：收到yuv数据，并编码成h264格式。123456789101112131415//打开编码器，就是在aw_x264基础上，封了一层。extern void aw_sw_encoder_open_x264_encoder(aw_x264_config *x264_config)&#123; if (aw_sw_x264_encoder_is_valid()) &#123; aw_log(&quot;[E] aw_sw_encoder_open_video_encoder when video encoder is not inited&quot;); return; &#125; int32_t x264_cfg_len = sizeof(aw_x264_config); if (!s_x264_config) &#123; s_x264_config = aw_alloc(x264_cfg_len); &#125; memcpy(s_x264_config, x264_config, x264_cfg_len); s_x264_ctx = alloc_aw_x264_context(*x264_config);&#125; 视频软编码器第二步：生成包含sps/pps数据的flv视频帧123456789101112131415161718192021222324//根据flv/h264/aac协议创建video/audio首帧tag，flv 格式相关代码在 aw_encode_flv.h/aw_encode_flv.c 中extern aw_flv_video_tag *aw_sw_encoder_create_x264_sps_pps_tag()&#123; if(!aw_sw_x264_encoder_is_valid())&#123; aw_log(&quot;[E] aw_sw_encoder_create_video_sps_pps_tag when video encoder is not inited&quot;); return NULL; &#125; //创建 sps pps // 创建flv视频tag aw_flv_video_tag *sps_pps_tag = aw_sw_encoder_create_flv_video_tag(); // 关键帧 sps_pps_tag-&gt;frame_type = aw_flv_v_frame_type_key; // package type 为header，固定 sps_pps_tag-&gt;h264_package_type = aw_flv_v_h264_packet_type_seq_header; // cts，项目内所有视频帧的cts 都为0 sps_pps_tag-&gt;h264_composition_time = 0; // 将aw_x264中生成的sps/pps数据copy到tag中 sps_pps_tag-&gt;config_record_data = copy_aw_data(s_x264_ctx-&gt;sps_pps_data); // 时间戳为0 sps_pps_tag-&gt;common_tag.timestamp = 0; // flv tag长度为：header size + data header(11字节) + 数据长度（后续介绍） sps_pps_tag-&gt;common_tag.data_size = s_x264_ctx-&gt;sps_pps_data-&gt;size + 11 + sps_pps_tag-&gt;common_tag.header_size; return sps_pps_tag;&#125; 视频软编码器第三步：将h264格式的数据转成flv视频数据。12345678910111213141516171819202122232425//将采集到的video yuv数据，编码为flv video tagextern aw_flv_video_tag * aw_sw_encoder_encode_x264_data(int8_t *yuv_data, long len, uint32_t timeStamp)&#123; //是否已开启编码 if (!aw_sw_x264_encoder_is_valid()) &#123; aw_log(&quot;[E] aw_sw_encoder_encode_video_data when video encoder is not inited&quot;); return NULL; &#125; //执行编码 aw_encode_yuv_frame_2_x264(s_x264_ctx, yuv_data, (int32_t)len); //编码后是否能取到数据 if (s_x264_ctx-&gt;encoded_h264_data-&gt;size &lt;= 0) &#123; return NULL; &#125; //将h264数据转成flv tag x264_picture_t *pic_out = s_x264_ctx-&gt;pic_out; aw_flv_video_tag *video_tag = aw_encoder_create_video_tag((int8_t *)s_x264_ctx-&gt;encoded_h264_data-&gt;data, s_x264_ctx-&gt;encoded_h264_data-&gt;size, timeStamp, (uint32_t)((pic_out-&gt;i_pts - pic_out-&gt;i_dts) * 1000.0 / s_x264_ctx-&gt;config.fps), pic_out-&gt;b_keyframe); ... return video_tag;&#125; 视频软编码器第四步：关闭编码器1234567891011121314151617//关闭编码器extern void aw_sw_encoder_close_x264_encoder()&#123; //避免重复关闭 if (!aw_sw_x264_encoder_is_valid()) &#123; aw_log(&quot;[E] aw_sw_encoder_close_video_encoder s_faac_ctx is NULL&quot;); return; &#125; //释放配置 if (s_x264_config) &#123; aw_free(s_x264_config); s_x264_config = NULL; &#125; //释放context free_aw_x264_context(&amp;s_x264_ctx);&#125; 至此，软编码代码介绍完毕。可以通过 AWSWFaacEncoder/AWSWX264Encoder 类调用上面的软编码器，给上层提供一致的接口。 总结，软编码器涉及的内容： 第三方编码器：libfaac/libx264 第三方编码器封装：aw_faac.h/aw_faac.c，aw_x264.h/aw_x264.c 编码器(将原始数据转成最终数据)封装：aw_sw_faac_encoder.h/aw_sw_faac_encoder.c，aw_sw_x264_encoder.h/aw_sw_x264_encoder.c 顶层抽象：AWSWFaacEncoder/AWSWX264Encoder 编码过程中需要注意的地方： 注意 audio specific config 及 sps/pps数据的获取，不获取这两种数据，服务器没办法识别音视频帧的。 faac编码后注意去除adts头部。 x264编码器如果输入分辨率的宽度不是16的倍数，需要将其扩展成16的倍数，否则编码可能会出问题(颜色丢失，uv混乱)。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里 ","slug":"1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","date":"2017-01-25T02:49:01.000Z","updated":"2018-07-19T07:13:13.681Z","comments":true,"path":"2017/01/25/1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里/","link":"","permalink":"https://hardman.github.io/2017/01/25/1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 我收到了很多反馈和评论，问我为什么AWLive运行不起来。大概是说报这样一个错误：我实在没想到会有人问这个问题。没想到的原因有2个： 这个问题不是一个技术问题，而是工具使用问题。我没想到会有人存在疑问。如果是培训机构出来的同学，应该第一天就会学这个。如果自学的同学，那学习能力肯定没得说，不可能代码写的好，工具却不会用。所以这部分刚入门没多久的初学者，或者是没有注意到工具使用的一些同学，被我忽略了，很抱歉，特写此文档说明。 我认为这个问题可能简单地搜索一下就能解决。所以有人问这个问题的时候，我没在意，也没办法回复。后续有好几个问这个，我才意识到可能是我的错误，应该当时制作工程时就排除此问题。 解决步骤注意:画圈的部分为点击／操作位置。请操作之前将手机连接到电脑 打开页面：https://github.com/hardman/AWLive 然后下载工程 解压 打开工程，点击Documentation 点击AWLive，右侧选择真机 选择正确的 code sign(开发者账号对应的证书名和profile，如果不清楚可自行查询) 点击运行。99%的情况下，可以正确运行。如遇到另外的1%，请留言。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 ","slug":"1小时学会：最简单的iOS直播推流（七）h264-aac-硬编码","date":"2016-12-07T14:02:59.000Z","updated":"2018-07-19T07:08:35.653Z","comments":true,"path":"2016/12/07/1小时学会：最简单的iOS直播推流（七）h264-aac-硬编码/","link":"","permalink":"https://hardman.github.io/2016/12/07/1小时学会：最简单的iOS直播推流（七）h264-aac-硬编码/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 前面已经介绍了如何从硬件设备获取到音视频数据（pcm，NV12）。 但是我们需要的视频格式是 aac和 h264。 现在就介绍一下如何将pcm编码aac，将NV12数据编码为h264。 编码分为软编码和硬编码。 硬编码是系统提供的，由系统专门嵌入的硬件设备处理音视频编码，主要计算操作在对应的硬件中。硬编码的特点是，速度快，cpu占用少，但是不够灵活，只能使用一些特定的功能。 软编码是指，通过代码计算进行数据编码，主要计算操作在cpu中。软编码的特点是，灵活，多样，功能丰富可扩展，但是cpu占用较多。 在代码中，编码器是通过AWEncoderManager获取的。 AWENcoderManager是一个工厂，通过audioEncoderType和videoEncoderType指定编码器类型。 编码器分为两类，音频编码器（AWAudioEncoder），视频编码器（AWVideoEncoder）。 音视频编码器又分别分为硬编码（在HW目录中）和软编码（在SW目录中）。 所以编码部分主要有4个文件：硬编码H264（AWHWH264Encoder），硬编码AAC（AWHWAACEncoder），软编码AAC（AWSWFaacEncoder），软编码H264（AWSWX264Encoder） 硬编码H264第一步，开启硬编码器12345678910111213141516171819202122232425262728293031-(void)open&#123; //创建 video encode session // 创建 video encode session // 传入视频宽高，编码类型：kCMVideoCodecType_H264 // 编码回调：vtCompressionSessionCallback，这个回调函数为编码结果回调，编码成功后，会将数据传入此回调中。 // (__bridge void * _Nullable)(self)：这个参数会被原封不动地传入vtCompressionSessionCallback中，此参数为编码回调同外界通信的唯一参数。 // &amp;_vEnSession，c语言可以给传入参数赋值。在函数内部会分配内存并初始化_vEnSession。 OSStatus status = VTCompressionSessionCreate(NULL, (int32_t)(self.videoConfig.pushStreamWidth), (int32_t)self.videoConfig.pushStreamHeight, kCMVideoCodecType_H264, NULL, NULL, NULL, vtCompressionSessionCallback, (__bridge void * _Nullable)(self), &amp;_vEnSession); if (status == noErr) &#123; // 设置参数 // ProfileLevel，h264的协议等级，不同的清晰度使用不同的ProfileLevel。 VTSessionSetProperty(_vEnSession, kVTCompressionPropertyKey_ProfileLevel, kVTProfileLevel_H264_Main_AutoLevel); // 设置码率 VTSessionSetProperty(_vEnSession, kVTCompressionPropertyKey_AverageBitRate, (__bridge CFTypeRef)@(self.videoConfig.bitrate)); // 设置实时编码 VTSessionSetProperty(_vEnSession, kVTCompressionPropertyKey_RealTime, kCFBooleanTrue); // 关闭重排Frame，因为有了B帧（双向预测帧，根据前后的图像计算出本帧）后，编码顺序可能跟显示顺序不同。此参数可以关闭B帧。 VTSessionSetProperty(_vEnSession, kVTCompressionPropertyKey_AllowFrameReordering, kCFBooleanFalse); // 关键帧最大间隔，关键帧也就是I帧。此处表示关键帧最大间隔为2s。 VTSessionSetProperty(_vEnSession, kVTCompressionPropertyKey_MaxKeyFrameInterval, (__bridge CFTypeRef)@(self.videoConfig.fps * 2)); // 关于B帧 P帧 和I帧，请参考：http://blog.csdn.net/abcjennifer/article/details/6577934 //参数设置完毕，准备开始，至此初始化完成，随时来数据，随时编码 status = VTCompressionSessionPrepareToEncodeFrames(_vEnSession); if (status != noErr) &#123; [self onErrorWithCode:AWEncoderErrorCodeVTSessionPrepareFailed des:@&quot;硬编码vtsession prepare失败&quot;]; &#125; &#125;else&#123; [self onErrorWithCode:AWEncoderErrorCodeVTSessionCreateFailed des:@&quot;硬编码vtsession创建失败&quot;]; &#125;&#125; 第二步，向编码器丢数据：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//这里的参数yuvData就是从相机获取的NV12数据。-(aw_flv_video_tag *)encodeYUVDataToFlvTag:(NSData *)yuvData&#123; if (!_vEnSession) &#123; return NULL; &#125; //yuv 变成 转CVPixelBufferRef OSStatus status = noErr; //视频宽度 size_t pixelWidth = self.videoConfig.pushStreamWidth; //视频高度 size_t pixelHeight = self.videoConfig.pushStreamHeight; //现在要把NV12数据放入 CVPixelBufferRef中，因为 硬编码主要调用VTCompressionSessionEncodeFrame函数，此函数不接受yuv数据，但是接受CVPixelBufferRef类型。 CVPixelBufferRef pixelBuf = NULL; //初始化pixelBuf，数据类型是kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange，此类型数据格式同NV12格式相同。 CVPixelBufferCreate(NULL, pixelWidth, pixelHeight, kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange, NULL, &amp;pixelBuf); // Lock address，锁定数据，应该是多线程防止重入操作。 if(CVPixelBufferLockBaseAddress(pixelBuf, 0) != kCVReturnSuccess)&#123; [self onErrorWithCode:AWEncoderErrorCodeLockSampleBaseAddressFailed des:@&quot;encode video lock base address failed&quot;]; return NULL; &#125; //将yuv数据填充到CVPixelBufferRef中 size_t y_size = pixelWidth * pixelHeight; size_t uv_size = y_size / 4; uint8_t *yuv_frame = (uint8_t *)yuvData.bytes; //处理y frame uint8_t *y_frame = CVPixelBufferGetBaseAddressOfPlane(pixelBuf, 0); memcpy(y_frame, yuv_frame, y_size); uint8_t *uv_frame = CVPixelBufferGetBaseAddressOfPlane(pixelBuf, 1); memcpy(uv_frame, yuv_frame + y_size, uv_size * 2); //硬编码 CmSampleBufRef //时间戳 uint32_t ptsMs = self.manager.timestamp + 1; //self.vFrameCount++ * 1000.f / self.videoConfig.fps; CMTime pts = CMTimeMake(ptsMs, 1000); //硬编码主要其实就这一句。将携带NV12数据的PixelBuf送到硬编码器中，进行编码。 status = VTCompressionSessionEncodeFrame(_vEnSession, pixelBuf, pts, kCMTimeInvalid, NULL, pixelBuf, NULL); ... ...&#125; 第三步，通过硬编码回调获取h264数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071static void vtCompressionSessionCallback (void * CM_NULLABLE outputCallbackRefCon, void * CM_NULLABLE sourceFrameRefCon, OSStatus status, VTEncodeInfoFlags infoFlags, CM_NULLABLE CMSampleBufferRef sampleBuffer )&#123; //通过outputCallbackRefCon获取AWHWH264Encoder的对象指针，将编码好的h264数据传出去。 AWHWH264Encoder *encoder = (__bridge AWHWH264Encoder *)(outputCallbackRefCon); //判断是否编码成功 if (status != noErr) &#123; dispatch_semaphore_signal(encoder.vSemaphore); [encoder onErrorWithCode:AWEncoderErrorCodeEncodeVideoFrameFailed des:@&quot;encode video frame error 1&quot;]; return; &#125; //是否数据是完整的 if (!CMSampleBufferDataIsReady(sampleBuffer)) &#123; dispatch_semaphore_signal(encoder.vSemaphore); [encoder onErrorWithCode:AWEncoderErrorCodeEncodeVideoFrameFailed des:@&quot;encode video frame error 2&quot;]; return; &#125; //是否是关键帧，关键帧和非关键帧要区分清楚。推流时也要注明。 BOOL isKeyFrame = !CFDictionaryContainsKey( (CFArrayGetValueAtIndex(CMSampleBufferGetSampleAttachmentsArray(sampleBuffer, true), 0)), kCMSampleAttachmentKey_NotSync); //首先获取sps 和pps //sps pss 也是h264的一部分，可以认为它们是特别的h264视频帧，保存了h264视频的一些必要信息。 //没有这部分数据h264视频很难解析出来。 //数据处理时，sps pps 数据可以作为一个普通h264帧，放在h264视频流的最前面。 BOOL needSpsPps = NO; if (!encoder.spsPpsData) &#123; if (isKeyFrame) &#123; //获取avcC，这就是我们想要的sps和pps数据。 //如果保存到文件中，需要将此数据前加上 [0 0 0 1] 4个字节，写入到h264文件的最前面。 //如果推流，将此数据放入flv数据区即可。 CMFormatDescriptionRef sampleBufFormat = CMSampleBufferGetFormatDescription(sampleBuffer); NSDictionary *dict = (__bridge NSDictionary *)CMFormatDescriptionGetExtensions(sampleBufFormat); encoder.spsPpsData = dict[@&quot;SampleDescriptionExtensionAtoms&quot;][@&quot;avcC&quot;]; &#125; needSpsPps = YES; &#125; //获取真正的视频帧数据 CMBlockBufferRef blockBuffer = CMSampleBufferGetDataBuffer(sampleBuffer); size_t blockDataLen; uint8_t *blockData; status = CMBlockBufferGetDataPointer(blockBuffer, 0, NULL, &amp;blockDataLen, (char **)&amp;blockData); if (status == noErr) &#123; size_t currReadPos = 0; //一般情况下都是只有1帧，在最开始编码的时候有2帧，取最后一帧 while (currReadPos &lt; blockDataLen - 4) &#123; uint32_t naluLen = 0; memcpy(&amp;naluLen, blockData + currReadPos, 4); naluLen = CFSwapInt32BigToHost(naluLen); //naluData 即为一帧h264数据。 //如果保存到文件中，需要将此数据前加上 [0 0 0 1] 4个字节，按顺序写入到h264文件中。 //如果推流，需要将此数据前加上4个字节表示数据长度的数字，此数据需转为大端字节序。 //关于大端和小端模式，请参考此网址：http://blog.csdn.net/hackbuteer1/article/details/7722667 encoder.naluData = [NSData dataWithBytes:blockData + currReadPos + 4 length:naluLen]; currReadPos += 4 + naluLen; encoder.isKeyFrame = isKeyFrame; &#125; &#125;else&#123; [encoder onErrorWithCode:AWEncoderErrorCodeEncodeGetH264DataFailed des:@&quot;got h264 data failed&quot;]; &#125; ... ...&#125; 第四步，其实，此时硬编码已结束，这一步跟编码无关，将取得的h264数据，送到推流器中。12345678910111213141516171819202122232425262728293031323334353637-(aw_flv_video_tag *)encodeYUVDataToFlvTag:(NSData *)yuvData&#123; ... ... if (status == noErr) &#123; dispatch_semaphore_wait(self.vSemaphore, DISPATCH_TIME_FOREVER); if (_naluData) &#123; //此处 硬编码成功，_naluData内的数据即为h264视频帧。 //我们是推流，所以获取帧长度，转成大端字节序，放到数据的最前面 uint32_t naluLen = (uint32_t)_naluData.length; //小端转大端。计算机内一般都是小端，而网络和文件中一般都是大端。大端转小端和小端转大端算法一样，就是字节序反转就行了。 uint8_t naluLenArr[4] = &#123;naluLen &gt;&gt; 24 &amp; 0xff, naluLen &gt;&gt; 16 &amp; 0xff, naluLen &gt;&gt; 8 &amp; 0xff, naluLen &amp; 0xff&#125;; //将数据拼在一起 NSMutableData *mutableData = [NSMutableData dataWithBytes:naluLenArr length:4]; [mutableData appendData:_naluData]; //将h264数据合成flv tag，合成flvtag之后就可以直接发送到服务端了。后续会介绍 aw_flv_video_tag *video_tag = aw_encoder_create_video_tag((int8_t *)mutableData.bytes, mutableData.length, ptsMs, 0, self.isKeyFrame); //到此，编码工作完成，清除状态。 _naluData = nil; _isKeyFrame = NO; CVPixelBufferUnlockBaseAddress(pixelBuf, 0); CFRelease(pixelBuf); return video_tag; &#125; &#125;else&#123; [self onErrorWithCode:AWEncoderErrorCodeEncodeVideoFrameFailed des:@&quot;encode video frame error&quot;]; &#125; CVPixelBufferUnlockBaseAddress(pixelBuf, 0); CFRelease(pixelBuf); return NULL; 第五步，关闭编码器1234567891011//永远不忘记关闭释放资源。-(void)close&#123; dispatch_semaphore_signal(self.vSemaphore); VTCompressionSessionInvalidate(_vEnSession); _vEnSession = nil; self.naluData = nil; self.isKeyFrame = NO; self.spsPpsData = nil;&#125; 硬编码AAC硬编码AAC逻辑同H264差不多。 第一步，打开编码器123456789101112131415161718192021222324252627282930313233343536373839404142434445-(void)open&#123; //创建audio encode converter也就是AAC编码器 //初始化一系列参数 AudioStreamBasicDescription inputAudioDes = &#123; .mFormatID = kAudioFormatLinearPCM, .mSampleRate = self.audioConfig.sampleRate, .mBitsPerChannel = (uint32_t)self.audioConfig.sampleSize, .mFramesPerPacket = 1,//每个包1帧 .mBytesPerFrame = 2,//每帧2字节 .mBytesPerPacket = 2,//每个包1帧也是2字节 .mChannelsPerFrame = (uint32_t)self.audioConfig.channelCount,//声道数，推流一般使用单声道 //下面这个flags的设置参照此文：http://www.mamicode.com/info-detail-986202.html .mFormatFlags = kLinearPCMFormatFlagIsPacked | kLinearPCMFormatFlagIsSignedInteger | kLinearPCMFormatFlagIsNonInterleaved, .mReserved = 0 &#125;; //设置输出格式，声道数 AudioStreamBasicDescription outputAudioDes = &#123; .mChannelsPerFrame = (uint32_t)self.audioConfig.channelCount, .mFormatID = kAudioFormatMPEG4AAC, 0 &#125;; //初始化_aConverter uint32_t outDesSize = sizeof(outputAudioDes); AudioFormatGetProperty(kAudioFormatProperty_FormatInfo, 0, NULL, &amp;outDesSize, &amp;outputAudioDes); OSStatus status = AudioConverterNew(&amp;inputAudioDes, &amp;outputAudioDes, &amp;_aConverter); if (status != noErr) &#123; [self onErrorWithCode:AWEncoderErrorCodeCreateAudioConverterFailed des:@&quot;硬编码AAC创建失败&quot;]; &#125; //设置码率 uint32_t aBitrate = (uint32_t)self.audioConfig.bitrate; uint32_t aBitrateSize = sizeof(aBitrate); status = AudioConverterSetProperty(_aConverter, kAudioConverterEncodeBitRate, aBitrateSize, &amp;aBitrate); //查询最大输出 uint32_t aMaxOutput = 0; uint32_t aMaxOutputSize = sizeof(aMaxOutput); AudioConverterGetProperty(_aConverter, kAudioConverterPropertyMaximumOutputPacketSize, &amp;aMaxOutputSize, &amp;aMaxOutput); self.aMaxOutputFrameSize = aMaxOutput; if (aMaxOutput == 0) &#123; [self onErrorWithCode:AWEncoderErrorCodeAudioConverterGetMaxFrameSizeFailed des:@&quot;AAC 获取最大frame size失败&quot;]; &#125;&#125; 第二步，获取audio specific config，这是一个特别的flv tag，存储了使用的aac的一些关键数据，作为解析音频帧的基础。在rtmp中，必须将此帧在所有音频帧之前发送。123456789101112131415161718192021222324-(aw_flv_audio_tag *)createAudioSpecificConfigFlvTag&#123; //profile，表示使用的协议 uint8_t profile = kMPEG4Object_AAC_LC; //采样率 uint8_t sampleRate = 4; //channel信息 uint8_t chanCfg = 1; //将上面3个信息拼在一起，成为2字节 uint8_t config1 = (profile &lt;&lt; 3) | ((sampleRate &amp; 0xe) &gt;&gt; 1); uint8_t config2 = ((sampleRate &amp; 0x1) &lt;&lt; 7) | (chanCfg &lt;&lt; 3); //将数据转成aw_data aw_data *config_data = NULL; data_writer.write_uint8(&amp;config_data, config1); data_writer.write_uint8(&amp;config_data, config2); //转成flv tag aw_flv_audio_tag *audio_specific_config_tag = aw_encoder_create_audio_specific_config_tag(config_data, &amp;_faacConfig); free_aw_data(&amp;config_data); //返回给调用方，准备发送 return audio_specific_config_tag;&#125; 第三步：当从麦克风获取到音频数据时，将数据交给AAC编码器编码。1234567891011121314151617181920212223242526272829303132333435363738394041424344-(aw_flv_audio_tag *)encodePCMDataToFlvTag:(NSData *)pcmData&#123; self.curFramePcmData = pcmData; //构造输出结构体，编码器需要 AudioBufferList outAudioBufferList = &#123;0&#125;; outAudioBufferList.mNumberBuffers = 1; outAudioBufferList.mBuffers[0].mNumberChannels = (uint32_t)self.audioConfig.channelCount; outAudioBufferList.mBuffers[0].mDataByteSize = self.aMaxOutputFrameSize; outAudioBufferList.mBuffers[0].mData = malloc(self.aMaxOutputFrameSize); uint32_t outputDataPacketSize = 1; //执行编码，此处需要传一个回调函数aacEncodeInputDataProc，以同步的方式，在回调中填充pcm数据。 OSStatus status = AudioConverterFillComplexBuffer(_aConverter, aacEncodeInputDataProc, (__bridge void * _Nullable)(self), &amp;outputDataPacketSize, &amp;outAudioBufferList, NULL); if (status == noErr) &#123; //编码成功，获取数据 NSData *rawAAC = [NSData dataWithBytes: outAudioBufferList.mBuffers[0].mData length:outAudioBufferList.mBuffers[0].mDataByteSize]; //时间戳(ms) = 1000 * 每秒采样数 / 采样率; self.manager.timestamp += 1024 * 1000 / self.audioConfig.sampleRate; //获取到aac数据，转成flv audio tag，发送给服务端。 return aw_encoder_create_audio_tag((int8_t *)rawAAC.bytes, rawAAC.length, (uint32_t)self.manager.timestamp, &amp;_faacConfig); &#125;else&#123; //编码错误 [self onErrorWithCode:AWEncoderErrorCodeAudioEncoderFailed des:@&quot;aac 编码错误&quot;]; &#125; return NULL;&#125;//回调函数，系统指定格式static OSStatus aacEncodeInputDataProc(AudioConverterRef inAudioConverter, UInt32 *ioNumberDataPackets, AudioBufferList *ioData, AudioStreamPacketDescription **outDataPacketDescription, void *inUserData)&#123; AWHWAACEncoder *hwAacEncoder = (__bridge AWHWAACEncoder *)inUserData; //将pcm数据交给编码器 if (hwAacEncoder.curFramePcmData) &#123; ioData-&gt;mBuffers[0].mData = (void *)hwAacEncoder.curFramePcmData.bytes; ioData-&gt;mBuffers[0].mDataByteSize = (uint32_t)hwAacEncoder.curFramePcmData.length; ioData-&gt;mNumberBuffers = 1; ioData-&gt;mBuffers[0].mNumberChannels = (uint32_t)hwAacEncoder.audioConfig.channelCount; return noErr; &#125; return -1;&#125; 第四步：关闭编码器释放资源123456-(void)close&#123; AudioConverterDispose(_aConverter); _aConverter = nil; self.curFramePcmData = nil; self.aMaxOutputFrameSize = 0;&#125; 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 ","slug":"1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍","date":"2016-11-24T14:43:38.000Z","updated":"2018-07-19T07:09:12.941Z","comments":true,"path":"2016/11/24/1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍/","link":"","permalink":"https://hardman.github.io/2016/11/24/1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 前面介绍了如何捕获音视频原始数据，介绍了yuv和pcm。 下面来介绍一下我们的想要转换的目标音视频格式：h264，aac，flv。 什么是h264？这里就不贴名词解释了。 说明一下，为什么需要这种格式。 其实除了h264格式之外，视频格式有很多种，出现这些格式原因无非有3种。 12345678910111213141516171. 压缩尺寸。我们来计算一下：yuv420格式，宽度为480，高度为320的视频，每一帧 需要 480*320*3/2 = 230400 字节。假设视频每秒20帧，那么30秒的视频 是 230400 * 20 * 30 = 138240000 字节 大约138M。你可能觉得不是很大，但是这个分辨率同样清晰度的h264视频可能只需要1-2M左右（当然也跟码率有关）。2. 为了与当前产品业务匹配，或者版权专利等原因，创造一种独特的格式。比如QuickTime，WMV等。3. 对当前技术的改进。h264正是对h263的改进。 h264 怎么压缩视频数据呢？ 最重要的一点是将视频帧分为关键帧和非关键帧。 关键帧的数据是完整的。包含了所有的颜色数据。这样的视频帧称为I帧。 非关键帧数据不完整，但是它能够根据前面或者后面的帧数据，甚至自己的部分帧数据，将自身数据补充完整。这种视频帧被称为 B/P 帧。 总体来说，h264跟yuv相比，最大的不同就是它是压缩的（通常此过程称为编码，不只是简单的压缩）。 什么是aacaac同h264性质一样，它也是pcm的压缩（编码）格式。 mp3大家都听说过，以前听歌保存到电脑里的歌曲90%都是mp3格式。 aac 相对 mp3来说，是更先进的压缩格式。 什么是flvh264是视频编码格式。 aac是音频编码格式。 除了这两种格式之外，还有一种将视频和音频合成(muxer 这个词会在相关代码中经常出现)在一起的格式。 比如：mp4，avi，rmvb，flv。 flv 是一种简单的视频合成格式。 它支持指定的音视频格式，如：h263，h264，VP6 及 AAC，MP3，Nellymoser等。 简单说来，flv的组成如下：1flv header + script tag + video tag + audio tag + ... + video tag + audio tag flv由 flv header 和无数的tag组成的。 flv header 内容是固定的。 一个tag就像是一个数组中的元素。是一个单独的储存了信息的数据块。 script tag 内存储了视频相关信息，如：宽高，码率，fps，文件大小，音频信息等等。 video tag 中 存储的是完整的视频压缩格式的一帧数据，如h264数据。 audio tag 中 存储的是完整的音频压缩格式的一帧数据，如 aac数据。 这样把所有数据拼接在一起，写入文件。这个文件就是flv格式。可以使用播放器播放了。 而flv刚好支持 h264 和 aac。 为什么介绍flv呢？ 因为rtmp协议所传输的视频流，就要求是flv格式。 所以，程序从相机和麦克风捕获到音视频数据后，分别转成 aac和h264格式的音视频帧。 然后将aac和h264音视频帧合成flv格式的视频后发送到rtmp服务器。客户端就可以播放我们推的流了。 注：上述内容不一定够精确，以容易理解为上。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取","slug":"1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取","date":"2016-11-16T15:17:40.000Z","updated":"2018-07-19T07:08:59.401Z","comments":true,"path":"2016/11/16/1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取/","link":"","permalink":"https://hardman.github.io/2016/11/16/1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 前面介绍了如何通过相机实时获取音视频数据。 我们接下来就需要了解获取到的数据到底是什么样的。 使用系统提供的接口获取到的音视频数据都保存在CMSampleBufferRef中。 使用GPUImamge获取到的音频数据为CMSampleBufferRef，获取到的视频格式为BGRA格式的二进制数据。 CMSampleBufferRef介绍这个结构在iOS中表示一帧音频/视频数据。 它里面包含了这一帧数据的内容和格式。 我们可以把它的内容取出来，提取出/转换成 我们想要的数据。 代表视频的CMSampleBufferRef中保存的数据是yuv420格式的视频帧(因为我们在视频输出设置中将输出格式设为：kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange)。 代表音频的CMSampleBufferRef中保存的数据是PCM格式的音频帧。 yuv是什么？NV12又是什么？视频是由一帧一帧的数据连接而成，而一帧视频数据其实就是一张图片。 yuv是一种图片储存格式，跟RGB格式类似。 RGB格式的图片很好理解，计算机中的大多数图片，都是以RGB格式存储的。 yuv中，y表示亮度，单独只有y数据就可以形成一张图片，只不过这张图片是灰色的。u和v表示色差(u和v也被称为：Cb－蓝色差，Cr－红色差)， 为什么要yuv？ 有一定历史原因，最早的电视信号，为了兼容黑白电视，采用的就是yuv格式。 一张yuv的图像，去掉uv，只保留y，这张图片就是黑白的。 而且yuv可以通过抛弃色差来进行带宽优化。 比如yuv420格式图像相比RGB来说，要节省一半的字节大小，抛弃相邻的色差对于人眼来说，差别不大。 一张yuv格式的图像，占用字节数为 (width height + (width height) / 4 + (width height) / 4) = (width height) 3 / 2一张RGB格式的图像，占用字节数为（width height） * 3 在传输上，yuv格式的视频也更灵活(yuv3种数据可分别传输)。 很多视频编码器最初是不支持rgb格式的。但是所有的视频编码器都支持yuv格式。 综合来讲，我们选择使用yuv格式，所以我们编码之前，首先将视频数据转成yuv格式。 我们这里使用的就是yuv420格式的视频。 yuv420也包含不同的数据排列格式：I420，NV12，NV21. 其格式分别如下，I420格式：y,u,v 3个部分分别存储：Y0,Y1…Yn,U0,U1…Un/2,V0,V1…Vn/2NV12格式：y和uv 2个部分分别存储：Y0,Y1…Yn,U0,V0,U1,V1…Un/2,Vn/2NV21格式：同NV12，只是U和V的顺序相反。 综合来说，除了存储顺序不同之外，上述格式对于显示来说没有任何区别。 使用哪种视频的格式，取决于初始化相机时设置的视频输出格式。设置为kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange时，表示输出的视频格式为NV12；设置为kCVPixelFormatType_420YpCbCr8Planar时，表示使用I420。 GPUImage设置相机输出数据时，使用的就是NV12. 为了一致，我们这里也选择NV12格式输出视频。 PCM是什么？脉冲编码调制，其实是将不规则的模拟信号转换成数字信号，这样就可以通过物理介质存储起来。 而声音也是一种特定频率（20-20000HZ）的模拟信号，也可以通过这种技术转换成数字信号，从而保存下来。 PCM格式，就是录制声音时，保存的最原始的声音数据格式。 相信你应该听说过wav格式的音频，它其实就是给PCM数据流加上一段header数据，就成为了wav格式。 而wav格式有时候之所以被称为无损格式，就是因为他保存的是原始pcm数据（也跟采样率和比特率有关）。 像我们耳熟能详的那些音频格式，mp3，aac等等，都是有损压缩，为了节约占用空间，在很少损失音效的基础上，进行最大程度的压缩。 所有的音频编码器，都支持pcm编码，而且录制的声音，默认也是PCM格式，所以我们下一步就是要获取录制的PCM数据。 从CMSampleBufferRef中提取yuv数据在前面的文章(使用系统接口捕获视频)中，初始化输出设备时，我们将输出的数据设置为kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange。因此在CMSampleBufferRef中保存的是yuv420(NV12)格式数据。通过下面的方法将CMSampleBufferRef转为yuv420(NV12)。12345678910111213141516171819202122232425262728293031323334// AWVideoEncoder.m文件-(NSData *) convertVideoSmapleBufferToYuvData:(CMSampleBufferRef) videoSample&#123; // 获取yuv数据 // 通过CMSampleBufferGetImageBuffer方法，获得CVImageBufferRef。 // 这里面就包含了yuv420(NV12)数据的指针 CVImageBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(videoSample); //表示开始操作数据 CVPixelBufferLockBaseAddress(pixelBuffer, 0); //图像宽度（像素） size_t pixelWidth = CVPixelBufferGetWidth(pixelBuffer); //图像高度（像素） size_t pixelHeight = CVPixelBufferGetHeight(pixelBuffer); //yuv中的y所占字节数 size_t y_size = pixelWidth * pixelHeight; //yuv中的uv所占的字节数 size_t uv_size = y_size / 2; uint8_t *yuv_frame = aw_alloc(uv_size + y_size); //获取CVImageBufferRef中的y数据 uint8_t *y_frame = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 0); memcpy(yuv_frame, y_frame, y_size); //获取CMVImageBufferRef中的uv数据 uint8_t *uv_frame = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, 1); memcpy(yuv_frame + y_size, uv_frame, uv_size); CVPixelBufferUnlockBaseAddress(pixelBuffer, 0); //返回数据 return [NSData dataWithBytesNoCopy:yuv_frame length:y_size + uv_size];&#125; 将GPUImage获取到的BGRA格式的图片转成yuv(NV12)格式1234567891011121314151617181920212223242526272829//AWGPUImageAVCapture.m文件-(void)newFrameReadyAtTime:(CMTime)frameTime atIndex:(NSInteger)textureIndex&#123; [super newFrameReadyAtTime:frameTime atIndex:textureIndex]; if(!self.capture || !self.capture.isCapturing)&#123; return; &#125; //将bgra转为yuv //图像宽度 int width = imageSize.width; //图像高度 int height = imageSize.height; //宽*高 int w_x_h = width * height; //yuv数据长度 = (宽 * 高) * 3 / 2 int yuv_len = w_x_h * 3 / 2; //yuv数据 uint8_t *yuv_bytes = malloc(yuv_len); //ARGBToNV12这个函数是libyuv这个第三方库提供的一个将bgra图片转为yuv420格式的一个函数。 //libyuv是google提供的高性能的图片转码操作。支持大量关于图片的各种高效操作，是视频推流不可缺少的重要组件，你值得拥有。 [self lockFramebufferForReading]; ARGBToNV12(self.rawBytesForImage, width * 4, yuv_bytes, width, yuv_bytes + w_x_h, width, width, height); [self unlockFramebufferAfterReading]; NSData *yuvData = [NSData dataWithBytesNoCopy:yuv_bytes length:yuv_len]; [self.capture sendVideoYuvData:yuvData];&#125; 从CMSampleBufferRef中提取PCM数据1234567891011121314151617// AWAudioEncoder.m 文件-(NSData *) convertAudioSmapleBufferToPcmData:(CMSampleBufferRef) audioSample&#123; //获取pcm数据大小 NSInteger audioDataSize = CMSampleBufferGetTotalSampleSize(audioSample); //分配空间 int8_t *audio_data = aw_alloc((int32_t)audioDataSize); //获取CMBlockBufferRef //这个结构里面就保存了 PCM数据 CMBlockBufferRef dataBuffer = CMSampleBufferGetDataBuffer(audioSample); //直接将数据copy至我们自己分配的内存中 CMBlockBufferCopyDataBytes(dataBuffer, 0, audioDataSize, audio_data); //返回数据 return [NSData dataWithBytesNoCopy:audio_data length:audioDataSize];&#125; 至此我们已经将捕获的视频数据转为了yuv420格式，将音频数据转为了pcm格式。 接下来就可以对这些数据进行各种编码了。编码完成后，就可以将数据发送给服务器了。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜","slug":"1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜","date":"2016-11-15T16:03:54.000Z","updated":"2018-07-19T07:09:28.845Z","comments":true,"path":"2016/11/16/1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜/","link":"","permalink":"https://hardman.github.io/2016/11/16/1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 上一篇文章介绍了如何使用系统方法捕获视频数据，但是更多的时候，为了使用美颜滤镜，我们会选择GPUImage来获取视频数据。 GPUImage是一个可以为录制视频添加实时滤镜的一个著名第三方库。 该框架大概原理是，使用OpenGL着色器对视频图像进行颜色处理，然后存到frameBuffer，之后可以对此数据再次处理。重复上述过程，即可达到多重滤镜效果。 具体实现不细说，这里简要介绍一下GPUImage的使用，如何美颜，如何获取音视频数据。 使用GPUImageGPUImage的主要代码在 AWGPUImageAVCapture 这个类中。 初始化AWAVCaptureManager对象时将captureType设为AWAVCaptureTypeGPUImage，就会自动调用AWGPUImageAVCapture类来捕获视频数据。 代码在 onInit 方法中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748-(void)onInit&#123; //摄像头初始化 // AWGPUImageVideoCamera 继承自 GPUImageVideoCamera。继承是为了获取音频数据，原代码中，默认情况下音频数据发送给了 audioEncodingTarget。 // 这个东西一看类型是GPUImageMovieWriter，应该是文件写入功能。果断覆盖掉processAudioSampleBuffer方法，拿到音频数据后自己处理。 // 音频就这样可以了，GPUImage主要工作还是在视频处理这里。 // 设置预览分辨率 self.captureSessionPreset是根据AWVideoConfig的设置，获取的分辨率。设置前置、后置摄像头。 _videoCamera = [[AWGPUImageVideoCamera alloc] initWithSessionPreset:self.captureSessionPreset cameraPosition:AVCaptureDevicePositionFront]; //开启捕获声音 [_videoCamera addAudioInputsAndOutputs]; //设置输出图像方向，可用于横屏推流。 _videoCamera.outputImageOrientation = UIInterfaceOrientationPortrait; //镜像策略，这里这样设置是最自然的。跟系统相机默认一样。 _videoCamera.horizontallyMirrorRearFacingCamera = NO; _videoCamera.horizontallyMirrorFrontFacingCamera = YES; //设置预览view _gpuImageView = [[GPUImageView alloc] initWithFrame:self.preview.bounds]; [self.preview addSubview:_gpuImageView]; //初始化美颜滤镜 _beautifyFilter = [[GPUImageBeautifyFilter alloc] init]; //相机获取视频数据输出至美颜滤镜 [_videoCamera addTarget:_beautifyFilter]; //美颜后输出至预览 [_beautifyFilter addTarget:_gpuImageView]; // 到这里我们已经能够打开相机并预览了。 // 因为要推流，除了预览之外，我们还要截取到视频数据。这就需要使用GPUImage中的GPUImageRawDataOutput，它能将美颜后的数据输出，便于我们处理后发送出去。 // AWGPUImageAVCaptureDataHandler继承自GPUImageRawDataOutput，从 newFrameReadyAtTime 方法中就可以获取到美颜后输出的数据。 // 输出的图片格式为BGRA。 _dataHandler = [[AWGPUImageAVCaptureDataHandler alloc] initWithImageSize:CGSizeMake(self.videoConfig.width, self.videoConfig.height) resultsInBGRAFormat:YES capture:self]; [_beautifyFilter addTarget:_dataHandler]; // 令AWGPUImageAVCaptureDataHandler实现AWGPUImageVideoCameraDelegate协议，并且让camera的awAudioDelegate指向_dataHandler对象。 // 将音频数据转到_dataHandler中处理。然后音视频数据就可以都在_dataHandler中处理了。 _videoCamera.awAudioDelegate = _dataHandler; //开始捕获视频 [self.videoCamera startCameraCapture]; //修改帧率 [self updateFps:self.videoConfig.fps];&#125; 美颜滤镜使用的是：https://github.com/Guikunzhi/BeautifyFaceDemo感谢Guikunzhi的分享。想了解美颜详细算法的同学，可以自行学习。 AWGPUImageAVCaptureDataHandler中音视频处理方法：1234567891011121314151617181920212223242526272829303132333435363738// 获取到音频数据，通过sendAudioSampleBuffer发送出去-(void)processAudioSample:(CMSampleBufferRef)sampleBuffer&#123; if(!self.capture || !self.capture.isCapturing)&#123; return; &#125; [self.capture sendAudioSampleBuffer:sampleBuffer];&#125;// 获取到视频数据，转换格式后，使用sendVideoYuvData 发送出去。-(void)newFrameReadyAtTime:(CMTime)frameTime atIndex:(NSInteger)textureIndex&#123; [super newFrameReadyAtTime:frameTime atIndex:textureIndex]; if(!self.capture || !self.capture.isCapturing)&#123; return; &#125; // GPUImage获取到的数据是BGRA格式。 // 而各种编码器最适合编码的格式还是yuv。 // 所以在此将BGRA格式的视频数据转成yuv格式。(后面会介绍yuv和pcm格式) // 将bgra转为yuv int width = imageSize.width; int height = imageSize.height; int w_x_h = width * height; // 1帧yuv数据长度为 宽x高 * 3 / 2 int yuv_len = w_x_h * 3 / 2; uint8_t *yuv_bytes = malloc(yuv_len); //使用libyuv库，做格式转换。libyuv中的格式都是大端(高位存高位，低位存低位)，而iOS设备是小端(高位存低位，低位存高位)，小端为BGRA，则大端为ARGB，所以这里使用ARGBToNV12。 //self.rawBytesForImage就是美颜后的图片数据，格式是BGRA。 //关于大端小端，请自行baidu。 [self lockFramebufferForReading]; ARGBToNV12(self.rawBytesForImage, width * 4, yuv_bytes, width, yuv_bytes + w_x_h, width, width, height); [self unlockFramebufferAfterReading]; NSData *yuvData = [NSData dataWithBytesNoCopy:yuv_bytes length:yuv_len]; //将获取到的yuv420数据发送出去 [self.capture sendVideoYuvData:yuvData];&#125; 至此，已经成功使用GPUImage获取视频，美颜，格式转换，准备发送数据。还是很简单的。 我们现在能够使用2种方法来获取音频数据，接下来会介绍音视频编码相关内容。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频数据","slug":"1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频数据","date":"2016-11-14T03:34:25.000Z","updated":"2018-07-19T07:08:42.795Z","comments":true,"path":"2016/11/14/1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频数据/","link":"","permalink":"https://hardman.github.io/2016/11/14/1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频数据/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 通过系统相机录制视频获取音视频数据，是推流的第一步。源码中提供2种获取音视频数据的方法：一是使用系统自带接口；二是使用GPUImage。 本篇首先介绍第一种。 网络上关于获取视频数据的代码有不少，但是为了方便代码阅读，这里简要介绍一下。 [注意]请仔细阅读代码注释 相关代码入口整套推流代码的入口：AWAVCaptureManager，它是根据参数创建上述2种获取数据方法的一个工厂类。 可以通过设置 captureType 来决定使用哪种数据获取方式。 AWAVCaptureManager部分代码如下：1234567891011121314typedef enum : NSUInteger &#123; AWAVCaptureTypeNone, AWAVCaptureTypeSystem, AWAVCaptureTypeGPUImage,&#125; AWAVCaptureType;@interface AWAVCaptureManager : NSObject//视频捕获类型@property (nonatomic, unsafe_unretained) AWAVCaptureType captureType;@property (nonatomic, weak) AWAVCapture *avCapture;//省略其他代码......@end 设置了captureType之后，直接可以通过avCapture获取到正确的捕获视频数据的对象了。 AWAVCapture 是一个虚基类（c++中的说法，不会直接产生对象，只用来继承的类，java中叫做抽象类）。它的两个子类分别是 AWSystemAVCapture 和 AWGPUImageAVCapture。 这里使用了多态。 如果 captureType设置的是 AWAVCaptureTypeSystem，avCapture获取到的真实对象就是 AWSystemAVCapture类型；如果 captureType设置的是 AWAVCaptureTypeGPUImage，avCapture获取到的真实对象就是 AWGPUImageAVCapture类型。 AWSystemAVCapture类的功能只有一个：调用系统相机，获取音视频数据。 相机数据获取的方法分为3步骤： 初始化输入输出设备。 创建AVCaptureSession，用来管理视频与数据的捕获。 创建预览UI。还包括一些其他功能： 切换摄像头 更改fps 在代码中对应的是 AWSystemAVCapture中的 onInit方法。只要初始化就会调用。 【注意】请仔细阅读下文代码中的注释 初始化输入设备1234567891011121314151617-(void) createCaptureDevice&#123; // 初始化前后摄像头 // 执行这几句代码后，系统会弹框提示：应用想要访问您的相机。请点击同意 // 另外iOS10 需要在info.plist中添加字段NSCameraUsageDescription。否则会闪退，具体请自行baidu。 NSArray *videoDevices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo]; self.frontCamera = [AVCaptureDeviceInput deviceInputWithDevice:videoDevices.firstObject error:nil]; self.backCamera =[AVCaptureDeviceInput deviceInputWithDevice:videoDevices.lastObject error:nil]; // 初始化麦克风 // 执行这几句代码后，系统会弹框提示：应用想要访问您的麦克风。请点击同意 // 另外iOS10 需要在info.plist中添加字段NSMicrophoneUsageDescription。否则会闪退，具体请自行baidu。 AVCaptureDevice *audioDevice = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeAudio]; self.audioInputDevice = [AVCaptureDeviceInput deviceInputWithDevice:audioDevice error:nil]; //省略其他代码 ...&#125; 初始化输出设备123456789101112131415161718192021222324-(void) createOutput&#123; //创建数据获取线程 dispatch_queue_t captureQueue = dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, 0); //视频数据输出 self.videoDataOutput = [[AVCaptureVideoDataOutput alloc] init]; //设置代理，需要当前类实现protocol：AVCaptureVideoDataOutputSampleBufferDelegate [self.videoDataOutput setSampleBufferDelegate:self queue:captureQueue]; //抛弃过期帧，保证实时性 [self.videoDataOutput setAlwaysDiscardsLateVideoFrames:YES]; //设置输出格式为 yuv420 [self.videoDataOutput setVideoSettings:@&#123; (__bridge NSString *)kCVPixelBufferPixelFormatTypeKey:@(kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange) &#125;]; //音频数据输出 self.audioDataOutput = [[AVCaptureAudioDataOutput alloc] init]; //设置代理，需要当前类实现protocol：AVCaptureAudioDataOutputSampleBufferDelegate [self.audioDataOutput setSampleBufferDelegate:self queue:captureQueue]; // AVCaptureVideoDataOutputSampleBufferDelegate 和 AVCaptureAudioDataOutputSampleBufferDelegate 回调方法名相同都是： // captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection // 最终视频和音频数据都可以在此方法中获取。&#125; 创建 captureSession123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// AVCaptureSession 创建逻辑很简单，它像是一个中介者，从音视频输入设备获取数据，处理后，传递给输出设备(数据代理/预览layer)。-(void) createCaptureSession&#123; //初始化 self.captureSession = [AVCaptureSession new]; //修改配置 [self.captureSession beginConfiguration]; //加入视频输入设备 if ([self.captureSession canAddInput:self.videoInputDevice]) &#123; [self.captureSession addInput:self.videoInputDevice]; &#125; //加入音频输入设备 if ([self.captureSession canAddInput:self.audioInputDevice]) &#123; [self.captureSession addInput:self.audioInputDevice]; &#125; //加入视频输出 if([self.captureSession canAddOutput:self.videoDataOutput])&#123; [self.captureSession addOutput:self.videoDataOutput]; [self setVideoOutConfig]; &#125; //加入音频输出 if([self.captureSession canAddOutput:self.audioDataOutput])&#123; [self.captureSession addOutput:self.audioDataOutput]; &#125; //设置预览分辨率 //这个分辨率有一个值得注意的点： //iphone4录制视频时 前置摄像头只能支持 480*640 后置摄像头不支持 540*960 但是支持 720*1280 //诸如此类的限制，所以需要写一些对分辨率进行管理的代码。 //目前的处理是，对于不支持的分辨率会抛出一个异常 //但是这样做是不够、不完整的，最好的方案是，根据设备，提供不同的分辨率。 //如果必须要用一个不支持的分辨率，那么需要根据需求对数据和预览进行裁剪，缩放。 if (![self.captureSession canSetSessionPreset:self.captureSessionPreset]) &#123; @throw [NSException exceptionWithName:@&quot;Not supported captureSessionPreset&quot; reason:[NSString stringWithFormat:@&quot;captureSessionPreset is [%@]&quot;, self.captureSessionPreset] userInfo:nil]; &#125; self.captureSession.sessionPreset = self.captureSessionPreset; //提交配置变更 [self.captureSession commitConfiguration]; //开始运行，此时，CaptureSession将从输入设备获取数据，处理后，传递给输出设备。 [self.captureSession startRunning];&#125; 创建预览UI123456789// 其实只有一句代码：CALayer layer = [AVCaptureVideoPreviewLayer layerWithSession:self.captureSession];// 它其实是 AVCaptureSession的一个输出方式而已。// CaptureSession会将从input设备得到的数据，处理后，显示到此layer上。// 我们可以将此layer变换后加入到任意UIView中。-(void) createPreviewLayer&#123; self.previewLayer = [AVCaptureVideoPreviewLayer layerWithSession:self.captureSession]; self.previewLayer.frame = self.preview.bounds; [self.preview.layer addSublayer:self.previewLayer];&#125; 切换摄像头1234567891011121314151617181920-(void)setVideoInputDevice:(AVCaptureDeviceInput *)videoInputDevice&#123; if ([videoInputDevice isEqual:_videoInputDevice]) &#123; return; &#125; //captureSession 修改配置 [self.captureSession beginConfiguration]; //移除当前输入设备 if (_videoInputDevice) &#123; [self.captureSession removeInput:_videoInputDevice]; &#125; //增加新的输入设备 if (videoInputDevice) &#123; [self.captureSession addInput:videoInputDevice]; &#125; //提交配置，至此前后摄像头切换完毕 [self.captureSession commitConfiguration]; _videoInputDevice = videoInputDevice;&#125; 设置fps12345678910111213141516171819-(void) updateFps:(NSInteger) fps&#123; //获取当前capture设备 NSArray *videoDevices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo]; //遍历所有设备（前后摄像头） for (AVCaptureDevice *vDevice in videoDevices) &#123; //获取当前支持的最大fps float maxRate = [(AVFrameRateRange *)[vDevice.activeFormat.videoSupportedFrameRateRanges objectAtIndex:0] maxFrameRate]; //如果想要设置的fps小于或等于做大fps，就进行修改 if (maxRate &gt;= fps) &#123; //实际修改fps的代码 if ([vDevice lockForConfiguration:NULL]) &#123; vDevice.activeVideoMinFrameDuration = CMTimeMake(10, (int)(fps * 10)); vDevice.activeVideoMaxFrameDuration = vDevice.activeVideoMinFrameDuration; [vDevice unlockForConfiguration]; &#125; &#125; &#125;&#125; 获取音视频数据1234567891011-(void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection&#123; if (self.isCapturing) &#123; if ([self.videoDataOutput isEqual:captureOutput]) &#123; //捕获到视频数据，通过sendVideoSampleBuffer发送出去，后续文章会解释接下来的详细流程。 [self sendVideoSampleBuffer:sampleBuffer]; &#125;else if([self.audioDataOutput isEqual:captureOutput])&#123; //捕获到音频数据，通过sendVideoSampleBuffer发送出去 [self sendAudioSampleBuffer:sampleBuffer]; &#125; &#125;&#125; 至此，我们达到了所有目标：能够录制视频，预览，获取音视频数据，切换前后摄像头，修改捕获视频的fps。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（二）代码架构概述","slug":"1小时学会：最简单的iOS直播推流（二）代码架构概述","date":"2016-11-10T15:33:21.000Z","updated":"2018-07-19T07:08:54.336Z","comments":true,"path":"2016/11/10/1小时学会：最简单的iOS直播推流（二）代码架构概述/","link":"","permalink":"https://hardman.github.io/2016/11/10/1小时学会：最简单的iOS直播推流（二）代码架构概述/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 推流流程使用rtmp协议（其他协议也类似）推流的大体流程如下： 通过系统相机捕获视频及声音，该美颜的美颜，该滤镜的滤镜。 捕获的视频帧为yuv格式，音频帧为pcm格式。 将捕获的音视频数据，传入一个串行队列(编码队列)，在队列中进行编码操作。 将yuv格式视频数据，转成h264格式视频帧；将pcm格式音频数据，转成aac格式音频帧。 将转好的h264及aac格式数据，转成flv视频帧。放入编码缓冲区，待发送。继续获取视频帧并编码。 建立rtmp连接到服务器，成功后，创建另一个串行队列（发送队列）。 rtmp协议，需要在首帧发送 sps/pps和AudioSpecificConfig这2种特别的帧数据。 发送了首帧之后，发送队列不停从编码队列中获取flv视频帧，发送至rtmp服务端。 结束直播，关闭推流，释放资源。 我的代码严格按照上述流程编写。这些逻辑也适用于市面上出现的几乎所有的推流代码。 我把上述流程及源代码画了2个图。里面有详细的流程及使用的技术。 推流流程图 代码结构类图 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"1小时学会：最简单的iOS直播推流（一）介绍","slug":"1小时学会：最简单的iOS直播推流（一）介绍","date":"2016-11-06T16:13:19.000Z","updated":"2018-07-19T07:08:24.471Z","comments":true,"path":"2016/11/07/1小时学会：最简单的iOS直播推流（一）介绍/","link":"","permalink":"https://hardman.github.io/2016/11/07/1小时学会：最简单的iOS直播推流（一）介绍/","excerpt":"","text":"最简单的iOS 推流代码，视频捕获，软编码(faac，x264)，硬编码（aac，h264），美颜，flv编码，rtmp协议，陆续更新代码解析，你想学的知识这里都有，愿意懂直播技术的同学快来看！！ 源代码：https://github.com/hardman/AWLive 介绍最近在做iOS直播，研究了相关直播技术，主要包含两方面：推流，播放。 因为之前使用cocos2dx做过一个视频游戏（恋爱公寓），用ffmpeg+sdl+cocos2dx实现过视频播放器。 游戏中的视频是hevc(h265)+aac合成mp4文件，使用aes加密。视频播放的时候，需要使用ffmpeg中的crypt模块进行aes解密后播放视频，解析出来的yuv图片数据直接送给OpenGL显示。 所以这次主要研究推流技术。并将代码开源。 其实直播技术中不论播放还是推流，更多的应该算是技术整合，就是将前人做好的协议和实现，整合成我们自己想要的功能。 而这次做这个项目也并不是做了什么技术创新，github里面已经有着很多直播源代码，可能比我写的更好更完整。而我的代码，特点就是简单直接，直奔主题。 我会在我的博客里做一些简单的解析，目的是希望让更多的人了解直播技术，能够了解直播内部的一些简单的原理，不再知其然不知其所以然。 功能范围 视频捕获：系统方法捕获，GPUImage捕获，CMSampleRef解析 美颜滤镜：GPUImage， 视频变换：libyuv 软编码：faac，x264 硬编码：VideoToolbox(aac/h264) libaw：C语言函数库 flv协议及编码 推流协议：librtmp，rtmp重连，rtmp各种状态回调 代码使用及注意代码使用方法见Demo。后续会根据上述功能的每一点对源代码进行解析。 如果有什么疑问或者问题，请评论指出。希望能够给愿意了解直播技术的人抛出一块好砖。 注1：项目中所有相关的文件名，类名，全局变量，全局方法都会加AW/aw作为前缀。 注2：项目中关键代码都使用c语言编写，理论上可以很容易地移植到android中。 文章列表 1小时学会：最简单的iOS直播推流（一）项目介绍 1小时学会：最简单的iOS直播推流（二）代码架构概述 1小时学会：最简单的iOS直播推流（三）使用系统接口捕获音视频 1小时学会：最简单的iOS直播推流（四）如何使用GPUImage，如何美颜 1小时学会：最简单的iOS直播推流（五）yuv、pcm数据的介绍和获取 1小时学会：最简单的iOS直播推流（六）h264、aac、flv介绍 1小时学会：最简单的iOS直播推流（七）h264/aac 硬编码 1小时学会：最简单的iOS直播推流（八）h264/aac 软编码 1小时学会：最简单的iOS直播推流（九）flv 编码与音视频时间戳同步 1小时学会：最简单的iOS直播推流（十）librtmp使用介绍 1小时学会：最简单的iOS直播推流（十一）sps&amp;pps和AudioSpecificConfig介绍（完结） 1小时学会：最简单的iOS直播推流（番外）运行不起AWLive的demo的同学请看这里","categories":[{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/categories/iOS推流/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"iOS推流","slug":"iOS推流","permalink":"https://hardman.github.io/tags/iOS推流/"},{"name":"源码","slug":"源码","permalink":"https://hardman.github.io/tags/源码/"},{"name":"教程","slug":"教程","permalink":"https://hardman.github.io/tags/教程/"},{"name":"github","slug":"github","permalink":"https://hardman.github.io/tags/github/"}]},{"title":"Adobe源码泄漏？3行代码搞定，Flash动画无缝导入Android/iOS/cocos2dx（二）","slug":"Adobe源码泄漏？3行代码搞定，Flash动画无缝导入Android-iOS-cocos2dx（二）","date":"2016-04-22T11:40:15.000Z","updated":"2018-07-19T06:24:37.009Z","comments":true,"path":"2016/04/22/Adobe源码泄漏？3行代码搞定，Flash动画无缝导入Android-iOS-cocos2dx（二）/","link":"","permalink":"https://hardman.github.io/2016/04/22/Adobe源码泄漏？3行代码搞定，Flash动画无缝导入Android-iOS-cocos2dx（二）/","excerpt":"","text":"[注] iOS代码已重构，效率提升90%，200层动画不卡。[2016.10.27] 上一篇 点此阅读 简要介绍了FlashToAnimation的功能，也就是将flash动画无缝导入到Android/iOS及cocos2dx中运行, 这一篇介绍这个库的使用方法。点此查看源码。 准备工作首先确保系统中安装了flash，并且flash版本应该在cs3或者以上。然后把”源码根目录/tools/flashScript”目录内的所有文件和文件夹copy到如下目录： Mac：~/Library/Application Support/Adobe/[Flash CS+版本号]/[en_US或者zh_CN]/Configuration/Commands Windows：C:\\Users[用户名]\\AppData\\Local\\Adobe[Flash CS+版本号][en_US或者zh_CN]\\Configuration\\Commands 在文件管理器（或Finder）目录中看起来是这样的： --Commands -- 1.根据png创建元件.jsfl -- 2.修改fla中元素的名字.jsfl -- 3.导出动画数据.jsfl -- libs/ --json2.jsfl -- ....其他文件 如图： 这时候打开flash，点击菜单栏中的 Commands（中文的话应该是命令），在下拉菜单中就能看到我们加入的脚本啦。 到此为止准备工作就绪。 美术人员制作flash动画的步骤下面步骤看起来很长，其实内容很简单，都是大家各自平时使用的经验，在这里写这么多是为了让小白用户不出错而已。美术人员使用步骤： 新建一个as3.0的Flash Document。 保存文档，请务必保存文档，否则脚本不生效，并按照如下规则命名：fla的命名应该以 “.” 分为3部分： 测试.test.fla第一部分：中文，对本文件的中文描述。（不重要，可以随意取。）第二部分：英文，表示本文件的英文标识符。（重要，在代码中会使用到这个关键字。）第三部分：后缀，默认即可不用管。（使用.fla即可。）其中第一部分中文可忽略。 在新建的Flash文件窗口右侧的Library栏中，点击右键，新建一个文件夹名为“pics”(注意，名字不能错，后面有类似的要求也要遵守)。 把制作flash的图片（png格式）拖入pics文件夹中。[!!!注意，所有的png图片必须带后缀.png否则会出错！] 点击commands中的脚本“1.根据png创建元件”。结果如图： 如果是cocos2dx中使用，为了避免Sprite Frame Cache重名，或者想要为图片生成跟本动画相关的独一无二的前缀，可以点击commands中的脚本“2.修改fla中元素的名字”。结果如下： iOS可能也有此问题。因为直接拖入xcode中的文件一般选择“create groups”，这个只是逻辑文件夹，如果其他文件夹内存在同名文件则会冲突。所以最好每次制作动画，添加png图片的时候，都执行一次脚本“2.修改fla中元素的名字“。 新建一个Movie clip（影片剪辑），取一个合适的名字。然后拖入anims文件夹中 双击该Movie clip，进入编辑模式，此时就可以使用eles文件夹中的Movie clip，制作动画了。制作动画的具体细节要求，见下面的要求。 制作完成后，保存，美术人员的工作就完成了。 美术人员制作flash动画完整要求 下面涉及名字的地方可以使用 英文字母，数字和下划线，不要用中文。 先制作动画所需要的图片，png/jpg格式的，所有的动画元素需要全部使用图片，不可以使用矢量图和文字等等。 图片命名尽量简单，以减少程序处理的数据量。 建立fla时，使用Action Script 3。 在库中建立3个文件夹，名字为：pics（图片），anims（动画的动作，比如idle, move等），eles（图片对应的元件）。对应的资源请在不同的文件夹中建立。 每张图片（pics）都需要生成一个元件（eles），不要把多张图片放在一个元件中。所以元件的数量应该同图片的数量是相同的。 所有的元件请使用 “影片剪辑”(movie clip), 不要使用 “按钮” 和 “图片”。 把制作好的png图片（只用png，不要用jpg或其它格式图片）导入到flash中，并拖进pics文件夹下面。 依次生成png图片对应的元件（影片剪辑），把图片拖到元件中。使图片居中。元件名字应该同图片的名字完全相同。这一步可以使用脚本（“1.根据png创建元件“）代替这个操作。 建立新的元件，还是使用”影片剪辑”(movie clip)，然后拖进anims文件夹中。这就是需要制作的动作了。 这时候，就可以使用eles(不要使用pics中的图片)中的元件在时间轴中制作动作了。 制作动作，帧的普通操作(关键帧关键帧之间的传统补间，只能使用传统补间)都可以使用，但是对关键帧的处理只支持以下几种：移动，缩放，旋转，倾斜，颜色叠加，透明度的变化 这5种变换。 不要使用除13条中描述的其他任何对关键帧的操作，比如滤镜，显示混合等。 不要使用缓动，不要使用补间动画时元件旋转等高端操作。如果某一帧某个元件不可见，可以通过设置它的透明度为0，或者插入空白关键帧来实现。 不要使用嵌套动画：就是说关键帧上最好只用eles中的元素来做，不要做好了一段动画，把这段动画作为关键帧使用。。 使用eles中的原件制作动画时，始终保持锚点的位置在原件的中央，否则会出现位置不对的问题。默认锚点是在中央的，不要手动去调整它。 最后，保存成fla就可以了。美术人员最终输出就是一个.fla文件。 程序人员使用美术制作好的动画程序拿到美术人员制作好的fla文件后，首先要进行一番检查，看看是否合格。所以需要确保程序员熟悉flash的页面和菜单，并了解一些简单的flash软件操作。 打开.fla文件。简单检查一下文件完成度。 是否3个文件夹都在(anims，pics，eles)。 是否动画文件都在anims文件夹内。 是否pics与eles内文件数量相同，并且一一对应，相对应的2个组件名字也要完全一致。 是否pics和eles内的组件名字都有.png后缀。 如果需要给关键帧添加事件，需要选中该关键帧（首先在timeline中选中关键帧，然后在主页面中选中该帧代表的图片，过程中最好隐藏timeline中的其它层），然后点击右侧与library同级的标签页properties。在第一行标有 &lt; Instance Name &gt; 的输入框，输入你的事件名，程序能够在播放到这一帧的时候，触发这个事件（在代码中，事件对应的字段为”mark”）。 事件添加完成后，选择菜单：Commands（命令）- “3.导出动画数据”。窗口底部同Timeline（时间轴）同级的Output（输出）栏中会显示脚本执行过程。 成功后，打开.fla文件所在的目录，即可看到”.flajson文件”和.fla同名”图片文件夹”（里面是图片）。 如果需要使用二进制动画描述文件，则需要把”.flajson文件”转为”.flabin文件”，这两个后缀也不能改。转换需要使用脚本”源码根目录/tools/JsonToBin.py”文件。这是一个python脚本。如果系统内没有python，则需要安装一个。然后打开命令行（mac中使用终端，Windows中可使用cmd）执行如下命令，执行后的.flabin就是转换成二进制后的文件。 1python 源码根目录/tools/JsonToBin.py [.flajson文件全路径] [.flabin文件全路径] 这时候可以把”.flajson文件”（或者 “.flabin文件”，二者使用其一即可，代码库内部处理，无需额外写代码判断）和”图片文件夹”放入程序指定目录就可以使用了。 cocos2dx可以放在资源目录中任意位置。代码初始化时需要指定目录。 Android需要将这2个文件放入 Assets文件夹的子文件夹flashAnims中。 iOS拖入xcode中，选择“copy if need”和“create groups”，点击确定。 程序员如何在代码中调用动画123456789101112//cocos2dx版本使用方法//包含头文件#include &quot;AnimNode.h&quot;using namespace windy;... ...//使用代码AnimNode *animNode = AnimNode::create();animNode-&gt;load(&quot;xxxx/flashFileName.flajson&quot;);animNode-&gt;play(&quot;animationName&quot;, WINDY_ANIMNODE_LOOP_FOREVER);//这里的animationName就是flash中anims文件夹内的动画名称superNode-&gt;addChild(animNode); 12345678910111213141516171819202122&lt;!--Android版本使用方法--&gt;&lt;!--Android还需要在manifest文件中添加权限，与demo中相同添加即可。不要忘记res/values目录中的flashview_attr.xml文件。 --&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;RelativeLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot; xmlns:tools=&quot;http://schemas.android.com/tools&quot; xmlns:FlashView=&quot;http://schemas.android.com/apk/res-auto&quot; &lt;!--!!!!!!注意这个要加--&gt; android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;match_parent&quot; tools:context=&quot;com.xcyo.yoyo.flashsupport.MainActivity&quot;&gt; &lt;com.flashanimation.view.FlashView android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;match_parent&quot; FlashView:flashDir=&quot;flashAnims&quot; FlashView:flashFileName=&quot;callTextAnim&quot; FlashView:defaultAnim=&quot;arriving1&quot; &lt;!--这里的defaultAnim就是flash中anims文件夹内的动画名称--&gt; FlashView:designDPI=&quot;326&quot; FlashView:loopTimes=&quot;0&quot; android:id=&quot;@+id/flashview&quot; /&gt;&lt;/RelativeLayout&gt; 12345678910//iOS版本使用方法#import &quot;FlashView.h&quot;... ...FlashView *flashView = [[FlashView alloc] initWithFlashName:@&quot;flashFileName&quot;];flashView.frame = self.view.frame;// CGRectMake(100, 100, 200, 500);flashView.backgroundColor = [UIColor clearColor];[superView addSubview:flashView];[flashView play:@&quot;animationName&quot; loopTimes:FOREVER];//这里的animationName就是flash中anims文件夹内的动画名称","categories":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/categories/iOS/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"动画","slug":"动画","permalink":"https://hardman.github.io/tags/动画/"},{"name":"flash","slug":"flash","permalink":"https://hardman.github.io/tags/flash/"},{"name":"android","slug":"android","permalink":"https://hardman.github.io/tags/android/"},{"name":"cocos2dx","slug":"cocos2dx","permalink":"https://hardman.github.io/tags/cocos2dx/"}]},{"title":"Adobe源码泄漏？3行代码搞定，Flash动画无缝导入Android/iOS/cocos2dx（一）","slug":"Adobe源码泄漏？3行代码搞定，Flash动画无缝导入Android-iOS-cocos2dx（一）","date":"2016-04-22T10:36:44.000Z","updated":"2018-07-19T06:24:05.986Z","comments":true,"path":"2016/04/22/Adobe源码泄漏？3行代码搞定，Flash动画无缝导入Android-iOS-cocos2dx（一）/","link":"","permalink":"https://hardman.github.io/2016/04/22/Adobe源码泄漏？3行代码搞定，Flash动画无缝导入Android-iOS-cocos2dx（一）/","excerpt":"","text":"[注] iOS代码已重构，效率提升90%，200层动画不卡。[2016.10.27] 项目介绍项目名称：FlashAnimationToMobile 源码。 使用方法点这里。 这是一个把flash中的关键帧动画(不是序列帧)导出，然后在iOS／Android原生应用中解析并播放的一个插件。除了原生App，它也能够支持Cocos2dx（3.x）。 对于Flash软件，则支持Flash CS3及以上版本及最新的Animate CC。 这个库能够满足游戏，App开发中90%的2D动画需求。它可以用来做游戏中的人物动画：走动，攻击，跳跃，闪避等，以及UI特效，升级，转场等。也可以用于App动画：秀场礼物，用户升级，活动礼包，等等。 这个库目前已在3个线上项目(2个游戏，一个App: android+iOS)中使用了。它最大的特点就是：原生，关键帧动画。 实际效果如下： flash: iOS: android: cocos2dx: 项目来由 最开始有把flash关键帧动画导出的想法是当初做cocos2dx开发游戏的时候。当时开发的一个游戏项目，模仿《刀塔传奇》的动画样式和战斗模式。了解的朋友应该知道，《刀塔传奇》里面有很多英雄，每个英雄都有很多个动作。一般情况下，这种复杂动画应该避免使用序列帧动画(对内存要求高)，而应该用更高效的关键帧动画。 而当初立项的时候，项目组的美术人员对flash比较熟悉，希望用flash来做各个英雄的动画。更能节约时间。而cocos2dx当时还不能直接导入flash动画。于是，作为程序的我，就需要查阅各种资料，想解决方案。于是就有了这个项目。 我们的游戏当初45个英雄，每个英雄9个动作，全部使用flash制作，并用这个库来播放动画。同屏20个英雄，无卡顿完美运行。 看下我们游戏制作的flash原图： 过了一年，我的主要工作从游戏转移到app。现在是在做秀场项目。后来大家觉得送礼物的特效不够炫，希望手机端送礼物时也能够有网页版的那种效果。 于是我就产生了把这个cocos2dx的动画库，移植到iOS和android中的想法。就是今天介绍的这个项目了。 用这个动画库来播放美术人员做出来的flash特效作为秀场礼物动画。可令礼物丰富多彩，不再单调。 代码实现及功能范围代码分为两部分： 1. flash／python脚本 2. 各平台（iOS/Android/cocos2dx）解析库。 其中flash脚本的部分参考的是这几篇文章： 应用Flash JavaScript API解析fla文件 jsfl参考文档1 jsfl参考文档2 python脚本是为了把json数据(.flajson文件)转换成二进制数据(.flabin文件)，进一步缩小文件体积，同时带有一定的加密效果。 其中cocos2dx的项目只支持二进制格式，Android和iOS版本支持json和二进制格式。 二进制格式的文件大小要比json格式小10倍左右，加载速度也比json快。 当初在cocos2dx中做程序实现的时候，我是完全把flash的运行机制在cocos2dx中复制了一遍。其中包含了，元件，层和关键帧的概念。到后面移植App的时候，我的思路发生了变化，我觉得把层的概念淡化，然后在任何一帧，把不同层的图片同时绘制。这种思路可能更简单一些。所以App的代码实现逻辑，同cocos2dx版本的代码有一定的区别。 项目中的代码是最简单的能够使用的版本，功能和限制如下： - 只能用图片，不能用矢量图 - 只能使用如下属性：位置，缩放，旋转，切变，颜色叠加，透明度变化。 - cocos2dx版本计算content size的部分没有实现。 - app版本我为不同分辨率手机做了适配，但是没有编写计算其size的方法，因为我没有用到。 - 上述两点如果有这种需求，则需要自行添加这部分代码 虽说简单，但是这个库已经能够满足90%的相关需求了。 如果感兴趣，可以通过阅读资料，为其增加矢量图，滤镜，遮罩等功能。这些都是可以实现的。 我在代码关键部分都加了注释。感兴趣的可以围观一下。帮忙指出错误。 FlashAnimationToMobile 点此进入。","categories":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/categories/iOS/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"动画","slug":"动画","permalink":"https://hardman.github.io/tags/动画/"},{"name":"flash","slug":"flash","permalink":"https://hardman.github.io/tags/flash/"},{"name":"android","slug":"android","permalink":"https://hardman.github.io/tags/android/"},{"name":"cocos2dx","slug":"cocos2dx","permalink":"https://hardman.github.io/tags/cocos2dx/"}]},{"title":"详解intrinsicContentSize及约束优先级contentHugging&contentCompressionResistance","slug":"详解intrinsicContentSize及约束优先级contentHugging-contentCompressionResistance","date":"2016-03-14T09:38:59.000Z","updated":"2018-07-19T06:20:13.090Z","comments":true,"path":"2016/03/14/详解intrinsicContentSize及约束优先级contentHugging-contentCompressionResistance/","link":"","permalink":"https://hardman.github.io/2016/03/14/详解intrinsicContentSize及约束优先级contentHugging-contentCompressionResistance/","excerpt":"","text":"在了解intrinsicContentSize之前，我们需要先了解2个概念： AutoLayout在做什么 约束优先级是什么意思。 如果不了解这两个概念，看intinsic content size没有任何意义。注：由于上面这几个概念都是针对UIView或其子类(UILabel，UIImageView等等)来说的。所以下文中都用UIView指代。 AutoLayout在做什么一个UIView想要显示在屏幕中，仅须有2个需要确定的元素，一是位置，二是大小。只要2者确定，UIView就可以正确显示，至于显示的内容，则由UIView自己决定（drawRect）。 没有AutoLayout的时候，我们需要通过 initWithFrame:(CGRect)这种方式来指定UIView的位置和大小。而使用AutoLayout的过程，就是通过约束来确定UIView的位置和大小的过程。约束优先级–为什么约束需要优先级？因为有的时候2个约束可能会有冲突。比如：有一个UIView 距离父UIView的左右距离都是0，这是2个约束，此时再给此UIView加一个宽度约束，比如指定宽度为100，那么就会产生约束冲突了。因为，这两种约束不可能同时存在，只能满足一个，那么满足谁呢？默认情况下给UIView加的这几个约束优先级都是1000，属于最高的优先级了，表示此约束必须满足。所以这种冲突不能被iOS所允许。此时就需要修改优先级了。把其中任意一个约束的优先级改为小于1000的值即可。&gt;iOS可以通过比较两个”相互冲突的约束”的优先级，从而忽略低优先级的某个约束，达到正确布局的目的。用鼠标选中宽度约束，然后在屏幕右侧的菜单中，修改优先级，如下图：这样就没有约束冲突了。因为如果一旦两个约束冲突，系统会自动忽略优先级低的约束。上面举的这个例子有些极端，因为上面两个约束都是确定的值，而且是绝对冲突。所以如果遇到这种情况，可能选择删掉某个约束更为合适。而约束优先级更多的时候用于解决模糊约束（相对于上面的确定值约束来说）的冲突的问题。比如有这样一个问题：- UIView1有四个约束：距离父UIView左和上确定，宽和高也确定。- UIView2在UIView1的下面，约束也有4个：上面距离UIView1确定，左面同UIView1对齐，同UIView1等高且等宽。此时这两个UIView应该像这样： 这是一个很普通的应用场景，假设我希望有这样一个效果： 我希望UIView2的宽度不能超过50。当UIView1宽度小于50的时候，二者等宽；当UIView1宽度大于50的时候，UIView2不受UIView1宽度的影响。于是我给UIView2加上一条约束：宽度&lt;=50。这时候冲突来了：因为UIView1的宽度是定好的，而UIView2和UIView1等宽。那么UIView2的宽度就是确定的。 很显然，分为两种情况（根据UIView1的宽度不同）： 若UIView1的宽度大于50，UIView2的宽度也一定大于50，这跟新加的限制宽度&lt;=50的约束是冲突的。 否则不冲突。 更糟糕的是，实际情况中，UIView1的宽度可能不是一个确定的值。它有可能会被页面中的其他View所影响，可能还会在运行时产生变化，并不能保证它的实际宽度一定小于50。所以，一旦产生约束冲突，可能就会对应用产生不确定的影响：可能显示错乱，也可能程序崩溃。 所以我们为了得到正确的结果，应该这样处理： 当UIView1宽度小于等于50的时候，约束不冲突，修改优先级与否都是一样结果。 当UIView1宽度大于50的时候，忽略等宽约束，也就是降低等宽约束优先级。 所以我们把等宽约束的优先级修改为999。上面两条都满足，问题解决。 说到模糊约束，content Hugging／content Compression Resistance就是2个UIView自带的模糊约束。而这两个约束存在的条件则是UIView必须指定了 Intrinsic Content Size。在了解这两个模糊约束之前，必须了解Intrinsic Content Size是什么东西。 Intrinsic Contenet SizeIntrinsic Content Size：固有大小。顾名思义，在AutoLayout中，它作为UIView的属性（不是语法上的属性），意思就是说我知道自己的大小，如果你没有为我指定大小，我就按照这个大小来。 比如：大家都知道在使用AutoLayout的时候，UILabel是不用指定尺寸大小的，只需指定位置即可，就是因为，只要确定了文字内容，字体等信息，它自己就能计算出大小来。 UILabel，UIImageView，UIButton等这些组件及某些包含它们的系统组件都有 Intrinsic Content Size 属性。也就是说，遇到这些组件，你只需要为其指定位置即可。大小就使用Intrinsic Content Size就行了。在代码中，上述系统控件都重写了UIView 中的 -(CGSize)intrinsicContentSize: 方法。并且在需要改变这个值的时候调用：invalidateIntrinsicContentSize 方法，通知系统这个值改变了。所以当我们在编写继承自UIView的自定义组件时，也想要有Intrinsic Content Size的时候，就可以通过这种方法来轻松实现。Intrinsic冲突–一个UIView有了 Intrinsic Content Size 之后，才可以只指定位置，而不用指定大小。并且才可能会触发上述两个约束。但是问题又来了，对于上述这种UIView来说，只指定位置而不指定大小，有的时候会有问题。我们用UILabel来举例吧（所有支持Intrinsic Content Size 的组件都有此问题）。2个UILabel，UILabel1（文字内容：UILabel1）和UILabel2（文字内容：UILabel2），其内容按照下面说明布局：- 2个UILabel距离上边栏为50点。- UILabel1与左边栏距离为10，UILabel2左面距离UILabel1为10点。因为都具有Intrinsic属性，所以不需要指定size。位置应该也明确了。 现在问题来了，再给UILabel2加一条约束，右侧距离右边栏为10点。 很明显，如果按照约束来布局，则没办法满足2个UIlabel都使用 Intrinsic Content Size，至少某个UILabel的宽度大于Intrinsic Content Size。这种情况，我们称之为2个组件之间的“Intrinsic冲突”。 解决“Intrinsic冲突”的方案有2种： 两个UIlabel都不使用Intrinsic Content Size。为两个UIlabel增加新的约束，来显式指定它们的大小。如：给2个UIlabel增加宽度和高度约束或等宽等高约束等等。 可以让其中一个UIlabel使用Intrinsic Content Size，另一个label则自动占用剩余的空间。这时候就需要用到 Content Hugging 和 Content Compression Resistance了！具体做法在下面介绍。 一句话总结“Intrinsic冲突”：两个或多个可以使用Intrinsic Content Size的组件，因为组件中添加的其他约束，而无法同时使用 intrinsic Content Size了。 content Hugging／content Compression Resistance首先，这两个概念都是UIView的属性。 假设两个组件产生了“Intrinsic冲突”： Content Hugging 约束（不想变大约束）表示：如果组件的此属性优先级比另一个组件此属性优先级高的话，那么这个组件就保持不变，另一个可以在需要拉伸的时候拉伸。属性分横向和纵向2个方向。 Content Compression Resistance 约束（不想变小约束）表示：如果组件的此属性优先级比另一个组件此属性优先级高的话，那么这个组件就保持不变，另一个可以在需要压缩的时候压缩。属性分横向和纵向2个方向。 意思很明显。上面UIlabel这个例子中，很显然，如果某个UILabel使用Intrinsic Content Size的时候，另一个需要拉伸。所以我们需要调整两个UILabel的 Content Hugging约束的优先级就可以啦。在这个页面可以调整优先级（拉到最下面）。 分别调整两个UILabel的 Content Hugging的优先级可以得到不同的结果： Content Compression Resistance 的情况就不多说了，原理相同。 在代码中修改UIView的这两个优先级12[label setContentHuggingPriority:UILayoutPriorityDefaultHigh forAxis:UILayoutConstraintAxisHorizontal];[label setContentCompressionResistancePriority: UILayoutPriorityDefaultHigh forAxis:UILayoutConstraintAxisHorizontal]; Priority是个enum：12345typedef float UILayoutPriority;static const UILayoutPriority UILayoutPriorityRequired NS_AVAILABLE_IOS(6_0) = 1000; // A required constraint. Do not exceed this.static const UILayoutPriority UILayoutPriorityDefaultHigh NS_AVAILABLE_IOS(6_0) = 750; // This is the priority level with which a button resists compressing its content.static const UILayoutPriority UILayoutPriorityDefaultLow NS_AVAILABLE_IOS(6_0) = 250; // This is the priority level at which a button hugs its contents horizontally.static const UILayoutPriority UILayoutPriorityFittingSizeLevel NS_AVAILABLE_IOS(6_0) = 50; // When you send -[UIView systemLayoutSizeFittingSize:], the size fitting most closely to the target size (the argument) is computed. UILayoutPriorityFittingSizeLevel is the priority level with which the view wants to conform to the target size in that computation. It&apos;s quite low. It is generally not appropriate to make a constraint at exactly this priority. You want to be higher or lower. Axis表示横向及纵向：1234typedef NS_ENUM(NSInteger, UILayoutConstraintAxis) &#123; UILayoutConstraintAxisHorizontal = 0, UILayoutConstraintAxisVertical = 1&#125;; 创建自定义具有 Intrinsic Content Size 功能的组件代码及注释如下：123456//IntrinsicView.h#import &lt;UIKit/UIKit.h&gt;@interface IntrinsicView : UIView@property (nonatomic) CGSize extendSize;@end 1234567891011121314151617181920212223242526272829303132333435//IntrinsicView.m#import &quot;IntrinsicView.h&quot;static bool closeIntrinsic = false;//测试关闭Intrinsic的影响@implementation IntrinsicView- (instancetype)init&#123; self = [super init]; if (self) &#123; //不兼容旧版Autoreizingmask，只使用AutoLayout //如果为YES，在AutoLayout中则会自动将view的frame和bounds属性转换为约束。 self.translatesAutoresizingMaskIntoConstraints = NO; &#125; return self;&#125;//当用户设置extendSize时，提示系统IntrinsicContentSize变化了。-(void)setExtendSize:(CGSize)extendSize&#123; _extendSize = extendSize; //如果不加这句话，在view显示之后（比如延时几秒），再设置extendSize不会有效果。 //本例中也就是testInvalidateIntrinsic的方法不会产生预期效果。 [self invalidateIntrinsicContentSize];&#125;//通过覆盖intrinsicContentSize函数修改View的Intrinsic的大小-(CGSize)intrinsicContentSize&#123; if (closeIntrinsic) &#123; return CGSizeMake(UIViewNoIntrinsicMetric, UIViewNoIntrinsicMetric); &#125; else &#123; return CGSizeMake(_extendSize.width, _extendSize.height); &#125;&#125;@end 123456789101112131415161718192021222324252627282930313233343536373839404142434445//测试代码#import &quot;ViewController.h&quot;#import &quot;newViewCtlViewController.h&quot;#import &quot;IntrinsicView.h&quot;@interface ViewController ()@end@implementation ViewController-(void)viewDidLoad&#123; [super viewDidLoad]; [self testIntrinsicView];&#125;+-(void) testIntrinsicView&#123; IntrinsicView *intrinsicView1 = [[IntrinsicView alloc] init]; intrinsicView1.extendSize = CGSizeMake(100, 100); intrinsicView1.backgroundColor = [UIColor greenColor]; [self.view addSubview:intrinsicView1]; [self.view addConstraints:@[ //距离superview上方100点 [NSLayoutConstraint constraintWithItem:intrinsicView1 attribute:NSLayoutAttributeTop relatedBy:NSLayoutRelationEqual toItem:self.view attribute:NSLayoutAttributeTop multiplier:1 constant:100], //距离superview左面10点 [NSLayoutConstraint constraintWithItem:intrinsicView1 attribute:NSLayoutAttributeLeft relatedBy:NSLayoutRelationEqual toItem:self.view attribute:NSLayoutAttributeLeft multiplier:1 constant:10], ]]; IntrinsicView *intrinsicView2 = [[IntrinsicView alloc] init]; intrinsicView2.extendSize = CGSizeMake(100, 30); intrinsicView2.backgroundColor = [UIColor redColor]; [self.view addSubview:intrinsicView2]; [self.view addConstraints:@[ //距离superview上方220点 [NSLayoutConstraint constraintWithItem:intrinsicView2 attribute:NSLayoutAttributeTop relatedBy:NSLayoutRelationEqual toItem:self.view attribute:NSLayoutAttributeTop multiplier:1 constant:220], //距离superview左面10点 [NSLayoutConstraint constraintWithItem:intrinsicView2 attribute:NSLayoutAttributeLeft relatedBy:NSLayoutRelationEqual toItem:self.view attribute:NSLayoutAttributeLeft multiplier:1 constant:10], ]]; [self performSelector:@selector(testInvalidateIntrinsic:) withObject:intrinsicView2 afterDelay:2];&#125;-(void) testInvalidateIntrinsic:(IntrinsicView *)view&#123; view.extendSize = CGSizeMake(100, 80);&#125;@end 代码效果如下：","categories":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/categories/iOS/"}],"tags":[{"name":"iOS","slug":"iOS","permalink":"https://hardman.github.io/tags/iOS/"},{"name":"autolayout","slug":"autolayout","permalink":"https://hardman.github.io/tags/autolayout/"}]}]}